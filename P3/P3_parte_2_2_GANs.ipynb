{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2Ubr-O3iQVy",
    "tags": []
   },
   "source": [
    "# **Práctica 3**: GANs\n",
    "# Alumno : José Javier Gutiérrez Gil\n",
    "# Parte 2_2: Ejercicios extra GANs simples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALUMNO: JOSÉ JAVIER GUTIÉRREZ GIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/tensorflow/examples.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pixCh8jmtmr"
   },
   "source": [
    "### Ejercicio EXTRA:\n",
    "**Busca algún dataset interesante y entrena el modelo de pix2pix:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un dataset interesante para entrenar el modelo de pix2pix es el \"**CamVid**\" dataset. Este dataset es un conjunto de datos de segmentación de imágenes que contiene imágenes y anotaciones de segmentación de escenas de conducción grabadas desde una cámara montada en un automóvil en movimiento.\n",
    "\n",
    "El dataset se compone de 701 imágenes de tamaño 960x720 en formato PNG y sus correspondientes imágenes de segmentación de tamaño 960x720 en formato PNG. Las imágenes originales se tomaron a una velocidad de 30 fps mientras se conducía por varias áreas de Cambridge, Reino Unido. Las imágenes de segmentación fueron creadas manualmente y contienen etiquetas para 32 clases diferentes, que incluyen carreteras, edificios, aceras, vehículos, peatones, árboles, señales de tráfico, entre otras.\n",
    "\n",
    "Cada imagen de segmentación contiene píxeles etiquetados con una de las 32 categorías presentes en la imagen original. El objetivo de entrenar un modelo de pix2pix en este conjunto de datos sería crear un modelo que pueda tomar como entrada una imagen sin procesar y genere una imagen de segmentación correspondiente, donde cada píxel de la imagen segmentada es etiquetado con una de las 32 categorías presentes en la imagen.\n",
    "\n",
    "Este conjunto de datos es útil para entrenar modelos de segmentación de imágenes en el contexto de la conducción autónoma y puede ser utilizado para crear modelos de pix2pix que generen segmentaciones precisas de las imágenes de la cámara de un automóvil.\n",
    "\n",
    "Para descargar el dataset de **CamVid**, podemos visitar su sitio web  (http://www0.cs.ucl.ac.uk/staff/G.Brostow/) y una explicación del dataset en  (http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid//). \n",
    "\n",
    "Para implementar el entrenamiento del modelo de pix2pix utilizando el dataset de **CamVid**, necesitamos seguir los pasos ya utilizados en apartados anteriores:\n",
    "\n",
    "1. - Preprocesamiento del dataset: Convertimos las imágenes a tensores y las normalizamos para tener valores en el rango [-1, 1]. También aplicamos técnicas de aumento de datos, como rotaciones aleatorias o recortes, para aumentar la variabilidad del dataset.\n",
    "\n",
    "2. - Definición del modelo: Creamos el modelo de pix2pix utilizando con nuestras propias funciones. Para poder obtener más detalles pequeños, lo que hacemos es aumentar la profundidad de la red del generador. Notar que la resolución de las imagenes en nuestro caso es 1024x768x3. El aumentar la profundidad nos ayudará a mejorar la calidad en este tipod e imagenes en pro de un pequeño aumento en el calculo. Además, tendremos que controlar un posible overfiting por dicho aumento.\n",
    "\n",
    "3. - Funciones de pérdida: Definimos las funciones de pérdida que se utilizarán para entrenar el modelo. En el caso de pix2pix, se utiliza una combinación de la pérdida de adversario (para entrenar el discriminador) y la pérdida de contenido (para entrenar el generador).\n",
    "\n",
    "4. - Entrenamiento: Entrenamos el modelo utilizando el dataset de **CamVid** y las funciones de pérdida definidas. Durante el entrenamiento, actualizamos los pesos del generador y el discriminador en función de las retroalimentaciones que recibimos del conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                                                                                import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'camVid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load (image_file, labeel_file):\n",
    "    # Read and decode an image file to a uint8 tensor\n",
    "    image = tf.io.read_file (image_file)\n",
    "    image = tf.io.decode_png (image)\n",
    "\n",
    "    label = tf.io.read_file (labeel_file)\n",
    "    label = tf.io.decode_png (label)  \n",
    " \n",
    "    input_image = label  \n",
    "    real_image  = image\n",
    "\n",
    "    # Convert both images to float32 tensors\n",
    "    input_image = tf.cast (input_image, tf.float32)\n",
    "    real_image  = tf.cast (real_image, tf.float32)\n",
    "\n",
    "    return input_image, real_image\n",
    "\n",
    "inp, re = load(str (PATH + '/train/0001TP_009210.png'), str (PATH + '/train_labels/0001TP_009210_L.png') )\n",
    "print (inp.shape)\n",
    "print (re.shape)\n",
    "# Casting to int for matplotlib to display the images\n",
    "plt.figure ()\n",
    "plt.imshow (inp / 255.0)\n",
    "plt.figure ()\n",
    "plt.imshow (re / 255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The facade training set consist of 400 images\n",
    "BUFFER_SIZE = 1042\n",
    "# The batch size of 1 produced better results for the U-Net in the original pix2pix experiment\n",
    "BATCH_SIZE  = 1\n",
    "# Each image is 256x256 in size\n",
    "IMG_WIDTH   = 720\n",
    "IMG_HEIGHT  = 960\n",
    "\n",
    "# resize \n",
    "RE_IMG_WIDTH   = 820  \n",
    "RE_IMG_HEIGHT  = 1042 #  \n",
    "\n",
    "# Resize to work with pix2pix arch\n",
    "M_IMG_WIDTH   = 768  \n",
    "M_IMG_HEIGHT  = 1024 \n",
    "\n",
    "OUTPUT_CHANNELS = 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(input_image, real_image, width, height):\n",
    "    input_image = tf.image.resize(input_image, [height, width],\n",
    "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    real_image = tf.image.resize(real_image, [height, width],\n",
    "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "    return input_image, real_image\n",
    "\n",
    "def random_crop(input_image, real_image, width, height):\n",
    "    \n",
    "    print (input_image.shape)\n",
    "    print (real_image.shape)\n",
    "    \n",
    "    stacked_image = tf.stack([input_image, real_image], axis=0)\n",
    "    cropped_image = tf.image.random_crop(\n",
    "                              stacked_image, size=[2, height, width, 3])\n",
    "    print (cropped_image[0].shape)\n",
    "    print (cropped_image[1].shape)\n",
    "    \n",
    "    return cropped_image[0], cropped_image[1]\n",
    "\n",
    "# Normalizing the images to [-1, 1]\n",
    "def normalize(input_image, real_image):\n",
    "    input_image = (input_image / 127.5) - 1\n",
    "    real_image = (real_image / 127.5) - 1\n",
    "\n",
    "    return input_image, real_image\n",
    "\n",
    "@tf.function()\n",
    "def random_jitter (input_image, real_image):\n",
    "    # Resizing to 286x286\n",
    "    input_image, real_image = resize(input_image, real_image, RE_IMG_WIDTH, RE_IMG_HEIGHT)\n",
    "    \n",
    "    # Random cropping back to 256x256\n",
    "    input_image, real_image = random_crop(input_image, real_image, M_IMG_WIDTH, M_IMG_HEIGHT)\n",
    "    \n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        # Random mirroring\n",
    "        input_image = tf.image.flip_left_right (input_image)\n",
    "        real_image  = tf.image.flip_left_right (real_image)\n",
    "\n",
    "    return input_image, real_image\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "rj_inp, rj_re = random_jitter (inp, re)\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.imshow(rj_inp / 255.0)\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.imshow(rj_re / 255.0)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_train (image_file, label_file):\n",
    "    input_image, real_image = load (image_file, label_file)\n",
    "    input_image, real_image = random_jitter (input_image, real_image)\n",
    "    input_image, real_image = normalize (input_image, real_image)\n",
    "\n",
    "    return input_image, real_image\n",
    "\n",
    "def load_image_test (image_file, label_file):\n",
    "    input_image, real_image = load (image_file, label_file)\n",
    "    input_image, real_image = resize (input_image, real_image,\n",
    "                                       IMG_HEIGHT, IMG_WIDTH)\n",
    "    input_image, real_image = normalize (input_image, real_image)\n",
    "\n",
    "    return input_image, real_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorios de entrenamiento y etiquetas de entrenamiento\n",
    "train_dir       = PATH + '/train/'\n",
    "train_label_dir = PATH + '/train_labels/'\n",
    "\n",
    "# Obtener rutas de todas las imágenes de entrenamiento\n",
    "train_image_paths       = [os.path.join(train_dir, fname) for fname in os.listdir(train_dir)]\n",
    "train_label_image_paths = [os.path.join(train_label_dir, fname) for fname in os.listdir(train_label_dir)]\n",
    "\n",
    "# Crear dataset de tensorflow a partir de las rutas de las imágenes\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_image_paths, train_label_image_paths))\n",
    "\n",
    "# Aplicar función de carga y preprocesamiento a todas las imágenes del dataset\n",
    "train_dataset = train_dataset.map(load_image_train)\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "try:\n",
    "    test_dir       = PATH + '/test/'\n",
    "    test_label_dir = PATH + '/test_labels/'\n",
    "\n",
    "    # Obtener rutas de todas las imágenes de entrenamiento\n",
    "    test_image_paths       = [os.path.join(test_dir, fname) for fname in os.listdir( test_dir)]\n",
    "    test_label_image_paths = [os.path.join(test_label_dir, fname) for fname in os.listdir( test_label_dir)]\n",
    "\n",
    "except tf.errors.InvalidArgumentError:\n",
    "    test_dir       = '/val/'\n",
    "    test_label_dir = '/val_labels/'\n",
    "\n",
    "    # Obtener rutas de todas las imágenes de entrenamiento\n",
    "    test_image_paths       = [os.path.join(test_dir, fname) for fname in os.listdir( test_dir)]\n",
    "    test_label_image_paths = [os.path.join(test_label_dir, fname) for fname in os.listdir( test_label_dir)]\n",
    "\n",
    "# Crear dataset de tensorflow a partir de las rutas de las imágenes\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_image_paths, test_label_image_paths))\n",
    "# Aplicar función de carga y preprocesamiento a todas las imágenes del dataset\n",
    "test_dataset = test_dataset.map(load_image_train)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIX2PIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "          tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                                     kernel_initializer=initializer, use_bias=False)\n",
    "    )\n",
    "\n",
    "    if apply_batchnorm:\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    result.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "down_model  = downsample (3, 4)\n",
    "down_result = down_model (tf.expand_dims (inp, 0))\n",
    "print (down_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(filters, size, apply_dropout=False):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "        tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                            padding='same',\n",
    "                                            kernel_initializer=initializer,\n",
    "                                            use_bias=False)\n",
    "    )\n",
    "\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    if apply_dropout:\n",
    "        result.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "    result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_model  = upsample (3, 4)\n",
    "up_result = up_model (down_result)\n",
    "print (up_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator():\n",
    "    '''\n",
    "    Original: \n",
    "        down_stack = [\n",
    "        downsample(64, 4, apply_batchnorm=False),  # (batch_size, 128, 128, 64)\n",
    "        downsample(128, 4),  # (batch_size, 64, 64, 128)\n",
    "        downsample(256, 4),  # (batch_size, 32, 32, 256)\n",
    "        downsample(512, 4),  # (batch_size, 16, 16, 512)\n",
    "        downsample(512, 4),  # (batch_size, 8, 8, 512)\n",
    "        downsample(512, 4),  # (batch_size, 4, 4, 512)\n",
    "        downsample(512, 4),  # (batch_size, 2, 2, 512)\n",
    "        downsample(512, 4),  # (batch_size, 1, 1, 512)\n",
    "    ]\n",
    "\n",
    "    up_stack = [\n",
    "        upsample(512, 4, apply_dropout=True),  # (batch_size, 2, 2, 1024)\n",
    "        upsample(512, 4, apply_dropout=True),  # (batch_size, 4, 4, 1024)\n",
    "        upsample(512, 4, apply_dropout=True),  # (batch_size, 8, 8, 1024)\n",
    "        upsample(512, 4),  # (batch_size, 16, 16, 1024)\n",
    "        upsample(256, 4),  # (batch_size, 32, 32, 512)\n",
    "        upsample(128, 4),  # (batch_size, 64, 64, 256)\n",
    "        upsample(64, 4),  # (batch_size, 128, 128, 128)\n",
    "    ]\n",
    "    '''\n",
    "    inputs = tf.keras.layers.Input(shape=[None, None, 3])\n",
    "    num_filters = 64 \n",
    " \n",
    "    down_stack = [\n",
    "        downsample(64, 4, apply_batchnorm=False),  # (batch_size, 128, 128, 64)\n",
    "        downsample(128, 4),  # (batch_size, 64, 64, 128)\n",
    "        downsample(256, 4),  # (batch_size, 32, 32, 256)\n",
    "        downsample(512, 4),  # (batch_size, 16, 16, 512)\n",
    "        downsample(512, 4),  # (batch_size, 8, 8, 512)\n",
    "        downsample(512, 4),  # (batch_size, 4, 4, 512)\n",
    "        downsample(512, 4),  # (batch_size, 2, 2, 512)\n",
    "        downsample(512, 4),  # (batch_size, 1, 1, 512)\n",
    "    ]\n",
    "\n",
    "    up_stack = [\n",
    "        upsample(512, 4, apply_dropout=True),  # (batch_size, 2, 2, 1024)\n",
    "        upsample(512, 4, apply_dropout=True),  # (batch_size, 4, 4, 1024)\n",
    "        upsample(512, 4, apply_dropout=True),  # (batch_size, 8, 8, 1024)\n",
    "        upsample(512, 4),  # (batch_size, 16, 16, 1024)\n",
    "        upsample(256, 4),  # (batch_size, 32, 32, 512)\n",
    "        upsample(128, 4),  # (batch_size, 64, 64, 256)\n",
    "        upsample(64, 4),  # (batch_size, 128, 128, 128)\n",
    "    ]\n",
    "\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
    "                                         strides=2,\n",
    "                                         padding='same',\n",
    "                                         kernel_initializer=initializer,\n",
    "                                         activation='tanh')  # \n",
    "\n",
    "    x = inputs\n",
    "\n",
    "    # Downsampling through the model\n",
    "    skips = []\n",
    "    for down in down_stack:\n",
    "        x = down(x)\n",
    "        skips.append(x)\n",
    "\n",
    "    skips = reversed(skips[:-1])\n",
    "\n",
    "    # Upsampling and establishing the skip connections\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        x = tf.keras.layers.Concatenate()([x, skip])\n",
    "\n",
    "   # x = tf.keras.layers.Conv2DTranspose(num_filters, 4, strides=1, padding='same',\n",
    "   #                                      kernel_initializer=tf.keras.initializers.RandomNormal(0., 0.02))(x)\n",
    "   #  x = tf.keras.layers.BatchNormalization()(x)\n",
    "   #  x = tf.keras.layers.ReLU()(x)\n",
    "    x = last(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Discriminator():\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    inp = tf.keras.layers.Input (shape=[None, None, 3], name='input_image')\n",
    "    tar = tf.keras.layers.Input (shape=[None, None, 3], name='target_image')\n",
    "\n",
    "    x = tf.keras.layers.concatenate([inp, tar])  # (batch_size, 256, 256, channels*2)\n",
    "\n",
    "    down1 = downsample(64, 4, False)(x)  # (batch_size, 128, 128, 64)\n",
    "    down2 = downsample(128, 4)(down1)  # (batch_size, 64, 64, 128)\n",
    "    down3 = downsample(256, 4)(down2)  # (batch_size, 32, 32, 256)\n",
    "\n",
    "    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)  # (batch_size, 34, 34, 256)\n",
    "    conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
    "                                kernel_initializer=initializer,\n",
    "                                use_bias=False)(zero_pad1)  # (batch_size, 31, 31, 512)\n",
    "\n",
    "    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
    "\n",
    "    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
    "\n",
    "    zero_pad2  = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (batch_size, 33, 33, 512)\n",
    "\n",
    "    last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
    "                                kernel_initializer=initializer)(zero_pad2)  # (batch_size, 30, 30, 1)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inp, tar], outputs=last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator ()\n",
    " \n",
    "tf.keras.utils.plot_model (generator, show_shapes=True, dpi=64)\n",
    "\n",
    "rj_inp, rj_re = random_jitter (inp, re)\n",
    "gen_output = generator (rj_inp [tf.newaxis, ...], training = False)\n",
    "plt.imshow (gen_output [0, ...])\n",
    "plt.show ()\n",
    "\n",
    "discriminator = Discriminator ()\n",
    " \n",
    "disc_out = discriminator ([rj_inp [tf.newaxis, ...], gen_output], training = False)\n",
    "plt.imshow (disc_out [0, ..., -1], vmin = -20, vmax = 20, cmap = 'RdBu_r')\n",
    "plt.colorbar ()\n",
    "plt.show () "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 100\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(disc_generated_output, gen_output, target):\n",
    "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "    # Mean absolute error\n",
    "    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "\n",
    "    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
    "\n",
    "    return total_gen_loss, gan_loss, l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
    "\n",
    "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "    return total_disc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the optimizers and a checkpoint-saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer     = tf.keras.optimizers.Adam (2e-4, beta_1 = 0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam (2e-4, beta_1 = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir    = './training_camvid_checkpoints'\n",
    "checkpoint_prefix = os.path.join (checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint (generator_optimizer = generator_optimizer,\n",
    "                                    discriminator_optimizer = discriminator_optimizer,\n",
    "                                    generator     = generator,\n",
    "                                    discriminator = discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images (model, test_input, tar):\n",
    "    prediction = model (test_input, training=True)\n",
    "    plt.figure(figsize = (15, 15))\n",
    "    prediction = (127.5 * prediction + 127.5).astype(np.uint8)\n",
    "    display_list = [test_input [0], tar [0], prediction [0]]\n",
    "    title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
    "\n",
    "    for i in range (3):\n",
    "        plt.subplot (1, 3, i+1)\n",
    "        plt.title (title [i])\n",
    "        # Getting the pixel values in the [0, 1] range to plot.\n",
    "        plt.imshow (display_list [i] * 0.5 + 0.5)\n",
    "        plt.axis ('off')\n",
    "    plt.show ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example_input, example_target in test_dataset.take (1):\n",
    "    generate_images (generator, example_input, example_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir=\"logs/\"\n",
    "\n",
    "summary_writer = tf.summary.create_file_writer(\n",
    "  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(input_image, target, step):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        gen_output = generator(input_image, training=True)\n",
    "\n",
    "        disc_real_output = discriminator([input_image, target], training=True)\n",
    "        disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
    "\n",
    "        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n",
    "        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
    "\n",
    "    generator_gradients = gen_tape.gradient(gen_total_loss,\n",
    "                                          generator.trainable_variables)\n",
    "    discriminator_gradients = disc_tape.gradient(disc_loss,\n",
    "                                               discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(generator_gradients,\n",
    "                                          generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
    "                                              discriminator.trainable_variables))\n",
    "\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('gen_total_loss', gen_total_loss, step=step//1000)\n",
    "        tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=step//1000)\n",
    "        tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=step//1000)\n",
    "        tf.summary.scalar('disc_loss', disc_loss, step=step//1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def fit (train_ds, test_ds, steps):\n",
    "    example_input, example_target = next (iter (test_ds.take (1)))\n",
    "    start = time.time()\n",
    "\n",
    "    for step, (input_image, target) in train_ds.repeat ().take (steps).enumerate ():\n",
    "        if (step) % 1000 == 0:\n",
    "            display.clear_output (wait=True)\n",
    "\n",
    "            if step != 0:\n",
    "                print (f'Time taken for 1000 steps: {time.time () - start:.2f} sec\\n')\n",
    "\n",
    "            start = time.time ()\n",
    "\n",
    "            generate_images (generator, example_input, example_target)\n",
    "            print (f\"Step: {step//1000}k\")\n",
    "\n",
    "        train_step (input_image, target, step)\n",
    "\n",
    "        # Training step\n",
    "        if (step+1) % 10 == 0:\n",
    "            print ('.', end = '', flush=True)\n",
    "\n",
    "\n",
    "        # Save (checkpoint) the model every 5k steps\n",
    "        if (step + 1) % 5000 == 0:\n",
    "            checkpoint.save (file_prefix = checkpoint_prefix)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit (train_dataset, test_dataset, steps = 40000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some images using the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the trained model on a few examples from the test set\n",
    "for inp, tar in test_dataset.take(15):\n",
    "    generate_images(generator, inp, tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fce33pmxnjlV"
   },
   "source": [
    "### Ejercicio EXTRA:\n",
    "Busca algún dataset interesante y entrena el modelo de cyclegan\n",
    "\n",
    "Una idea interesante podría ser entrenar un CycleGAN para convertir cuadros de vangogh en fotos y viceversa. Para ello, podemos utilizar el conjunto de datos \"vangogh2phto\" disponible en Tensorflow.\n",
    "\n",
    "Seguimos los mismos pasos que en el apartao de evaluación del modelo cycleGan:\n",
    "\n",
    "- Carga y procesamiento de las imagnes.\n",
    "\n",
    "- Creación modelo cycleGan \n",
    "\n",
    "- checkpoint para salvar los pesos de la red entrenada y así poder cargarlos donde se quedaron la última vez\n",
    "\n",
    "- Entrenamiento de la red con los dataset de train\n",
    "\n",
    "- Evaluación del modelo entrenado con los datasets de test.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    " \n",
    "import pathlib\n",
    " \n",
    "import datetime\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'vangogh2photo'  # @param [apple2orange, summer2winter_yosemite, horse2zebra, monet2photo, cezanne2photo, ukiyoe2photo, vangogh2photo, maps, cityscapes, facades, iphone2dslr_flower, ae_photos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_URL = f'https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/{dataset_name}.zip'\n",
    "\n",
    "#_URL = f'http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/{dataset_name}.tar.gz'\n",
    "#_URL = f'https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/{dataset_name}.zip'\n",
    "\n",
    "path_to_zip = tf.keras.utils.get_file (fname = f\"{dataset_name}.tar.gz\",\n",
    "                                        origin  = _URL,\n",
    "                                        extract = True)\n",
    "\n",
    "path_to_zip  = pathlib.Path (path_to_zip)\n",
    "PATH = path_to_zip.parent/dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (PATH)\n",
    "list  (PATH.parent.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = tf.io.read_file(str(PATH / 'trainA/00001.jpg'))\n",
    "sample_image = tf.io.decode_jpeg(sample_image)\n",
    "\n",
    "print (sample_image.shape)\n",
    "\n",
    "plt.figure ()\n",
    "plt.imshow (sample_image)\n",
    "\n",
    "sample_image = tf.io.read_file(str(PATH / 'trainB/2013-12-02 11_26_22.jpg'))\n",
    "sample_image = tf.io.decode_jpeg(sample_image)\n",
    "\n",
    "print (sample_image.shape)\n",
    "\n",
    "plt.figure ()\n",
    "plt.imshow (sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Directorios de entrenamiento y etiquetas de entrenamiento\n",
    "trainA_dir  = PATH / 'trainA'\n",
    "trainB_dir  = PATH / 'trainB'\n",
    "\n",
    "# cargar y preprocesar los datasets de imágenes\n",
    "trainA_s = tf.data.Dataset.list_files (str (trainA_dir / '*.jpg'))\n",
    "trainB_s = tf.data.Dataset.list_files (str (trainB_dir / '*.jpg'))\n",
    "\n",
    "print(len(trainA_s))\n",
    "print(len(trainB_s))\n",
    "\n",
    "# Directorios de entrenamiento y etiquetas de entrenamiento\n",
    "testA_dir = PATH / 'testA'\n",
    "testB_dir = PATH / 'testB'\n",
    "\n",
    "# cargar y preprocesar los datasets de imágenes\n",
    "testA_s = tf.data.Dataset.list_files (str (testA_dir / '*.jpg'))\n",
    "testB_s = tf.data.Dataset.list_files (str (testB_dir / '*.jpg'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load (image_file):\n",
    "    # Read and decode an image file to a uint8 tensor\n",
    "    image = tf.io.read_file (image_file)\n",
    "    image = tf.io.decode_jpeg (image, channels = 3)\n",
    "\n",
    "    # Convert both images to float32 tensors\n",
    "    input_image = tf.cast (image, tf.float32)\n",
    "\n",
    "    return input_image\n",
    "\n",
    "print (\"------------- TRAIN Images ----------\")\n",
    "\n",
    "i_TA = load (str (PATH / 'trainA/00001.jpg') )\n",
    "i_TB = load (str (PATH / 'trainB/2013-12-02 11_26_22.jpg') )\n",
    "print (i_TA.shape)\n",
    "print (i_TB.shape)\n",
    "# Casting to int for matplotlib to display the images\n",
    "plt.figure ()\n",
    "plt.imshow (i_TA / 255.0)\n",
    "plt.figure ()\n",
    "plt.imshow (i_TB / 255.0)\n",
    "plt.show()\n",
    "plt.close ()\n",
    "\n",
    "print (\"------------- TEST Images ----------\")\n",
    "\n",
    "i_TA = load (str (PATH / 'testA/00001.jpg') )\n",
    "i_TB = load (str (PATH / 'testB/2014-08-01 17_41_55.jpg') )\n",
    "print (i_TA.shape)\n",
    "print (i_TB.shape)\n",
    "# Casting to int for matplotlib to display the images\n",
    "plt.figure ()\n",
    "plt.imshow (i_TA / 255.0)\n",
    "\n",
    "plt.figure ()\n",
    "plt.imshow (i_TB / 255.0)\n",
    "plt.show()\n",
    "plt.close ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE_TRAIN = 400\n",
    "BUFFER_SIZE_TEST  = 6287\n",
    "\n",
    "BATCH_SIZE  = 1\n",
    "\n",
    "IMG_WIDTH   = 256\n",
    "IMG_HEIGHT  = 256\n",
    "\n",
    "R_IMG_WIDTH   = 286\n",
    "R_IMG_HEIGHT  = 286\n",
    "\n",
    "def random_crop (image):\n",
    "    cropped_image = tf.image.random_crop (\n",
    "                          image, size = [IMG_HEIGHT, IMG_WIDTH, 3])\n",
    "\n",
    "    return cropped_image\n",
    "\n",
    "# normalizing the images to [-1, 1]\n",
    "def normalize (image):\n",
    "    image = tf.cast (image, tf.float32)\n",
    "    image = (image / 127.5) - 1\n",
    "    \n",
    "    return image\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "OUTPUT_CHANNELS = 3\n",
    "\n",
    "generator_g = pix2pix.unet_generator (OUTPUT_CHANNELS, norm_type = 'instancenorm')\n",
    "generator_f = pix2pix.unet_generator (OUTPUT_CHANNELS, norm_type = 'instancenorm')\n",
    "\n",
    "discriminator_x = pix2pix.discriminator (norm_type = 'instancenorm', target=False)\n",
    "discriminator_y = pix2pix.discriminator (norm_type = 'instancenorm', target=False)\n",
    "\n",
    "sample_A = next(iter(trainA))\n",
    "sample_B = next(iter(trainB))\n",
    " \n",
    "to_B = generator_g (sample_A)\n",
    "to_B = to_B / 255.0\n",
    "to_A = generator_f (sample_B)\n",
    "to_A = to_A / 255.0\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "contrast = 8\n",
    "\n",
    "imgs = [sample_A, to_B, sample_B, to_A]\n",
    "title = ['picture', 'To photo', 'photo', 'To picture']\n",
    "\n",
    "for i in range(len(imgs)):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.title(title[i])\n",
    "    print (np.min (imgs[i][0]))\n",
    "    print (np.max (imgs[i][0]))\n",
    "    img = ((imgs[i][0] + 1) * 127.5).numpy().astype(np.uint8)\n",
    "    if i % 2 == 0:\n",
    "\n",
    "        plt.imshow(img)\n",
    "    else:\n",
    "        plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title('Is a real photo?')\n",
    "plt.imshow(discriminator_y(sample_B)[0, ..., -1], cmap='RdBu_r')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Is a real picture?')\n",
    "plt.imshow(discriminator_x(sample_A)[0, ..., -1], cmap='RdBu_r')\n",
    "\n",
    "plt.show ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 10\n",
    "\n",
    "loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real, generated):\n",
    "    real_loss = loss_obj(tf.ones_like(real), real)\n",
    "\n",
    "    generated_loss = loss_obj(tf.zeros_like(generated), generated)\n",
    "\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "    return total_disc_loss * 0.5\n",
    "\n",
    "def generator_loss(generated):\n",
    "    return loss_obj(tf.ones_like(generated), generated)\n",
    "\n",
    "def calc_cycle_loss(real_image, cycled_image):\n",
    "    loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
    "\n",
    "    return LAMBDA * loss1\n",
    "\n",
    "def identity_loss(real_image, same_image):\n",
    "    loss = tf.reduce_mean(tf.abs(real_image - same_image))\n",
    "    return LAMBDA * 0.5 * loss\n",
    "\n",
    "generator_g_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "generator_f_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "discriminator_x_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/vangogh2photo/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(generator_g=generator_g,\n",
    "                           generator_f=generator_f,\n",
    "                           discriminator_x=discriminator_x,\n",
    "                           discriminator_y=discriminator_y,\n",
    "                           generator_g_optimizer=generator_g_optimizer,\n",
    "                           generator_f_optimizer=generator_f_optimizer,\n",
    "                           discriminator_x_optimizer=discriminator_x_optimizer,\n",
    "                           discriminator_y_optimizer=discriminator_y_optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "def generate_images(model, test_input):\n",
    "    prediction = model(test_input)\n",
    "    contrast = 8\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    \n",
    "    display_list = [test_input[0]* 0.5 + 0.5, prediction[0]* 0.5 * contrast + 0.5]\n",
    "    title = ['Input Image', 'Predicted Image']\n",
    "\n",
    "    for i in range (2):\n",
    "        plt.subplot(1, 2, i+1)\n",
    "        plt.title(title[i])\n",
    "        # normalize pixel values to [0, 1]\n",
    "        img = display_list[i]\n",
    "        img = (img - np.min(img)) / (np.max(img) - np.min(img))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "@tf.function\n",
    "def train_step(real_x, real_y):\n",
    "    # persistent is set to True because the tape is used more than\n",
    "    # once to calculate the gradients.\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # Generator G translates X -> Y\n",
    "        # Generator F translates Y -> X.\n",
    "\n",
    "        fake_y = generator_g(real_x, training=True)\n",
    "        cycled_x = generator_f(fake_y, training=True)\n",
    "\n",
    "        \n",
    "        fake_x = generator_f(real_y, training=True)\n",
    "        cycled_y = generator_g(fake_x, training=True)\n",
    "\n",
    "        # same_x and same_y are used for identity loss.\n",
    "        same_x = generator_f(real_x, training=True)\n",
    "        same_y = generator_g(real_y, training=True)\n",
    "\n",
    "        disc_real_x = discriminator_x(real_x, training=True)\n",
    "        disc_real_y = discriminator_y(real_y, training=True)\n",
    "\n",
    "        disc_fake_x = discriminator_x(fake_x, training=True)\n",
    "        disc_fake_y = discriminator_y(fake_y, training=True)\n",
    "\n",
    "        # calculate the loss\n",
    "        gen_g_loss = generator_loss(disc_fake_y)\n",
    "        gen_f_loss = generator_loss(disc_fake_x)\n",
    "\n",
    "        total_cycle_loss = calc_cycle_loss(real_x, cycled_x) + calc_cycle_loss(real_y, cycled_y)\n",
    "\n",
    "        # Total generator loss = adversarial loss + cycle loss\n",
    "        total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_loss(real_y, same_y)\n",
    "        total_gen_f_loss = gen_f_loss + total_cycle_loss + identity_loss(real_x, same_x)\n",
    "\n",
    "        disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)\n",
    "        disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n",
    "  \n",
    "    # Calculate the gradients for generator and discriminator\n",
    "    generator_g_gradients = tape.gradient(total_gen_g_loss, \n",
    "                                        generator_g.trainable_variables)\n",
    "    generator_f_gradients = tape.gradient(total_gen_f_loss, \n",
    "                                        generator_f.trainable_variables)\n",
    "\n",
    "    discriminator_x_gradients = tape.gradient(disc_x_loss, \n",
    "                                            discriminator_x.trainable_variables)\n",
    "    discriminator_y_gradients = tape.gradient(disc_y_loss, \n",
    "                                            discriminator_y.trainable_variables)\n",
    "\n",
    "    # Apply the gradients to the optimizer\n",
    "    generator_g_optimizer.apply_gradients(zip(generator_g_gradients, \n",
    "                                            generator_g.trainable_variables))\n",
    "\n",
    "    generator_f_optimizer.apply_gradients(zip(generator_f_gradients, \n",
    "                                            generator_f.trainable_variables))\n",
    "\n",
    "    discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients,\n",
    "                                                discriminator_x.trainable_variables))\n",
    "\n",
    "    discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients,\n",
    "                                                discriminator_y.trainable_variables))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    n = 0\n",
    "    for image_x, image_y in tf.data.Dataset.zip((trainA, trainB)):\n",
    "        train_step (image_x, image_y)\n",
    "        if n % 10 == 0:\n",
    "            print ('.', end='')\n",
    "        n += 1\n",
    "\n",
    "    clear_output (wait=True)\n",
    "        # Using a consistent image (sample_horse) so that the progress of the model\n",
    "        # is clearly visible.\n",
    "    generate_images (generator_g, sample_A)\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                                 ckpt_save_path))\n",
    "\n",
    "    print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n",
    "                                                              time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the trained model on the test dataset\n",
    "for inp in testA.take (5):\n",
    "    generate_images (generator_g, inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the trained model on the test dataset\n",
    "for inp in testB.take(5):\n",
    "    generate_images(generator_f, inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnmaWaIhnnNr"
   },
   "source": [
    "### Ejercicio EXTRA:\n",
    "Busca algún dataset interesante y entrena el modelo de pix2pix y cyclegan y compara los resultados.\n",
    "\n",
    "Para comparar los resultados de pix2pix y CycleGAN, podemos utilizar el conjunto de datos \"**cityscapes**\" presente en los ejemplos de tensorflow con cyclegan. Además tenemos un directorio con las imagenes para luego implementar el modelo pix2pix.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Test to Pix2pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data: Log Cityscapes\n",
    "\n",
    "  - Dataset alacenado en directorio del notebook:\n",
    "                   path\\\n",
    "                        |\n",
    "                        cityscapes\\\n",
    "                                  |\n",
    "                                  train\\\n",
    "                                        1.jpg\n",
    "                                        ......\n",
    "                                  |\n",
    "                                   val\\\n",
    "                                       1.jpg\n",
    "                                        ...                               \n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'cityscapes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(image_file):\n",
    "    # Read and decode an image file to a uint8 tensor\n",
    "    image = tf.io.read_file(image_file)\n",
    "    image = tf.io.decode_jpeg(image)\n",
    "\n",
    "    # Split each image tensor into two tensors:\n",
    "    # - one with a real building facade image\n",
    "    # - one with an architecture label image \n",
    "    w = tf.shape(image)[1]\n",
    "    w = w // 2\n",
    "    input_image = image[:, w:, :]\n",
    "    real_image = image[:, :w, :]\n",
    "\n",
    "    # Convert both images to float32 tensors\n",
    "    input_image = tf.cast(input_image, tf.float32)\n",
    "    real_image = tf.cast(real_image, tf.float32)\n",
    "\n",
    "    return input_image, real_image\n",
    "\n",
    "inp, re = load(str(PATH + '/train/1.jpg'))\n",
    "print (inp.shape)\n",
    "print (re.shape)\n",
    "# Casting to int for matplotlib to display the images\n",
    "plt.figure()\n",
    "plt.imshow(inp / 255.0)\n",
    "plt.figure()\n",
    "plt.imshow(re / 255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The facade training set consist of 2975 images\n",
    "BUFFER_SIZE = 2975\n",
    "# The batch size of 1 produced better results for the U-Net in the original pix2pix experiment\n",
    "BATCH_SIZE  = 1\n",
    "# Each image is 256x256 in size\n",
    "IMG_WIDTH   = 256\n",
    "IMG_HEIGHT  = 256\n",
    "\n",
    "# resize\n",
    "RE_IMG_WIDTH   = 286  \n",
    "RE_IMG_HEIGHT  = 286  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(input_image, real_image, height, width):\n",
    "    input_image = tf.image.resize(input_image, [height, width],\n",
    "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    real_image = tf.image.resize(real_image, [height, width],\n",
    "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "    return input_image, real_image\n",
    "def random_crop(input_image, real_image):\n",
    "    stacked_image = tf.stack([input_image, real_image], axis=0)\n",
    "    cropped_image = tf.image.random_crop(\n",
    "                                  stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
    "\n",
    "    return cropped_image[0], cropped_image[1]\n",
    "\n",
    "# Normalizing the images to [-1, 1]\n",
    "def normalize(input_image, real_image):\n",
    "    input_image = (input_image / 127.5) - 1\n",
    "    real_image  = (real_image / 127.5) - 1\n",
    "\n",
    "    return input_image, real_image\n",
    "@tf.function()\n",
    "def random_jitter(input_image, real_image):\n",
    "    # Resizing to 286x286\n",
    "    input_image, real_image = resize(input_image, real_image, RE_IMG_WIDTH, RE_IMG_HEIGHT)\n",
    "\n",
    "    # Random cropping back to 256x256\n",
    "    input_image, real_image = random_crop(input_image, real_image)\n",
    "\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        # Random mirroring\n",
    "        input_image = tf.image.flip_left_right(input_image)\n",
    "        real_image = tf.image.flip_left_right(real_image)\n",
    "\n",
    "    return input_image, real_image\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "for i in range(4):\n",
    "    rj_inp, rj_re = random_jitter(inp, re)\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.imshow(rj_inp / 255.0)\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    rj_inp, rj_re = random_jitter(inp, re)\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.imshow(rj_re / 255.0)\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_train(image_file):\n",
    "    input_image, real_image = load(image_file)\n",
    "    input_image, real_image = random_jitter(input_image, real_image)\n",
    "    input_image, real_image = normalize(input_image, real_image)\n",
    "\n",
    "    return input_image, real_image\n",
    "\n",
    "def load_image_test(image_file):\n",
    "    input_image, real_image = load(image_file)\n",
    "    input_image, real_image = resize(input_image, real_image,\n",
    "                                   IMG_HEIGHT, IMG_WIDTH)\n",
    "    input_image, real_image = normalize(input_image, real_image)\n",
    "\n",
    "    return input_image, real_image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.list_files(str(PATH + '/train/*.jpg'))\n",
    "train_dataset = train_dataset.map(load_image_train,\n",
    "                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "try:\n",
    "    test_dataset = tf.data.Dataset.list_files(str(PATH + '/val/*.jpg'))\n",
    "except tf.errors.InvalidArgumentError:\n",
    "    print ('Error al cargar las imagenes de validación')\n",
    "test_dataset = test_dataset.map (load_image_test)\n",
    "test_dataset = test_dataset.batch (BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pix2pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 3\n",
    "\n",
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                             kernel_initializer=initializer, use_bias=False)\n",
    "    )\n",
    "\n",
    "    if apply_batchnorm:\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    result.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    return result\n",
    "\n",
    "def upsample(filters, size, apply_dropout=False):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "        tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                        padding='same',\n",
    "                                        kernel_initializer=initializer,\n",
    "                                        use_bias=False)\n",
    "    )\n",
    "\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    if apply_dropout:\n",
    "        result.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "    result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "down_model = downsample(3, 4)\n",
    "down_result = down_model(tf.expand_dims(inp, 0))\n",
    "print (down_result.shape)\n",
    "\n",
    "up_model  = upsample(3, 4)\n",
    "up_result = up_model(down_result)\n",
    "print (up_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator():\n",
    "    inputs = tf.keras.layers.Input(shape=[256, 256, 3])\n",
    "\n",
    "    down_stack = [\n",
    "        downsample(64, 4, apply_batchnorm=False),  # (batch_size, 128, 128, 64)\n",
    "        downsample(128, 4),  # (batch_size, 64, 64, 128)\n",
    "        downsample(256, 4),  # (batch_size, 32, 32, 256)\n",
    "        downsample(512, 4),  # (batch_size, 16, 16, 512)\n",
    "        downsample(512, 4),  # (batch_size, 8, 8, 512)\n",
    "        downsample(512, 4),  # (batch_size, 4, 4, 512)\n",
    "        downsample(512, 4),  # (batch_size, 2, 2, 512)\n",
    "        downsample(512, 4),  # (batch_size, 1, 1, 512)\n",
    "    ]\n",
    "\n",
    "    up_stack = [\n",
    "        upsample(512, 4, apply_dropout=True),  # (batch_size, 2, 2, 1024)\n",
    "        upsample(512, 4, apply_dropout=True),  # (batch_size, 4, 4, 1024)\n",
    "        upsample(512, 4, apply_dropout=True),  # (batch_size, 8, 8, 1024)\n",
    "        upsample(512, 4),  # (batch_size, 16, 16, 1024)\n",
    "        upsample(256, 4),  # (batch_size, 32, 32, 512)\n",
    "        upsample(128, 4),  # (batch_size, 64, 64, 256)\n",
    "        upsample(64, 4),  # (batch_size, 128, 128, 128)\n",
    "    ]\n",
    "\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
    "                                             strides=2,\n",
    "                                             padding='same',\n",
    "                                             kernel_initializer=initializer,\n",
    "                                             activation='tanh')  # (batch_size, 256, 256, 3)\n",
    "\n",
    "    x = inputs\n",
    "\n",
    "    # Downsampling through the model\n",
    "    skips = []\n",
    "    for down in down_stack:\n",
    "        x = down(x)\n",
    "        skips.append(x)\n",
    "\n",
    "    skips = reversed(skips[:-1])\n",
    "\n",
    "    # Upsampling and establishing the skip connections\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        x = tf.keras.layers.Concatenate()([x, skip])\n",
    "\n",
    "    x = last(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "generator = Generator()\n",
    "tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rj_inp, rj_re = random_jitter(inp, re)  # --123--  \n",
    "print(rj_inp.shape)\n",
    "gen_output = generator(rj_inp[tf.newaxis, ...], training=False) # --123--  \n",
    "plt.imshow(gen_output[0, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Discriminator():\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')\n",
    "    tar = tf.keras.layers.Input(shape=[256, 256, 3], name='target_image')\n",
    "\n",
    "    x = tf.keras.layers.concatenate([inp, tar])  # (batch_size, 256, 256, channels*2)\n",
    "\n",
    "    down1 = downsample(64, 4, False)(x)  # (batch_size, 128, 128, 64)\n",
    "    down2 = downsample(128, 4)(down1)  # (batch_size, 64, 64, 128)\n",
    "    down3 = downsample(256, 4)(down2)  # (batch_size, 32, 32, 256)\n",
    "\n",
    "    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)  # (batch_size, 34, 34, 256)\n",
    "    conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
    "                                kernel_initializer=initializer,\n",
    "                                use_bias=False)(zero_pad1)  # (batch_size, 31, 31, 512)\n",
    "\n",
    "    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
    "\n",
    "    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
    "\n",
    "    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (batch_size, 33, 33, 512)\n",
    "\n",
    "    last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
    "                                kernel_initializer=initializer)(zero_pad2)  # (batch_size, 30, 30, 1)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inp, tar], outputs=last)\n",
    "\n",
    "discriminator = Discriminator()\n",
    "tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rj_inp, rj_re = random_jitter(inp, re)  # --123--   \n",
    "disc_out = discriminator([rj_inp[tf.newaxis, ...], gen_output], training=False)\n",
    "plt.imshow(disc_out[0, ..., -1], vmin=-20, vmax=20, cmap='RdBu_r')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 100\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "def generator_loss(disc_generated_output, gen_output, target):\n",
    "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "    # Mean absolute error\n",
    "    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "\n",
    "    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
    "\n",
    "    return total_gen_loss, gan_loss, l1_loss\n",
    "\n",
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
    "\n",
    "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "    return total_disc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimicer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer     = tf.keras.optimizers.Adam (2e-4, beta_1 = 0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam (2e-4, beta_1 = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './checkpoints/city/raining'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir=\"logs/\"\n",
    "\n",
    "summary_writer = tf.summary.create_file_writer (\n",
    "                                  log_dir + \"fit_maps/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(model, test_input, tar):\n",
    "    prediction = model(test_input, training=True)\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    display_list = [test_input[0], tar[0], prediction[0]]\n",
    "    title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
    "\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        plt.title(title[i])\n",
    "        # Getting the pixel values in the [0, 1] range to plot.\n",
    "        plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "for example_input, example_target in test_dataset.take(1):\n",
    "    generate_images(generator, example_input, example_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(input_image, target, step):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        gen_output = generator(input_image, training=True)\n",
    "\n",
    "        disc_real_output = discriminator([input_image, target], training=True)\n",
    "        disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
    "\n",
    "        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n",
    "        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
    "\n",
    "    generator_gradients = gen_tape.gradient(gen_total_loss,\n",
    "                                          generator.trainable_variables)\n",
    "    discriminator_gradients = disc_tape.gradient(disc_loss,\n",
    "                                               discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(generator_gradients,\n",
    "                                          generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
    "                                              discriminator.trainable_variables))\n",
    "\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('gen_total_loss', gen_total_loss, step=step//1000)\n",
    "        tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=step//1000)\n",
    "        tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=step//1000)\n",
    "        tf.summary.scalar('disc_loss', disc_loss, step=step//1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(train_ds, test_ds, steps):\n",
    "    example_input, example_target = next(iter(test_ds.take(1)))\n",
    "    start = time.time()\n",
    "\n",
    "    for step, (input_image, target) in train_ds.repeat().take(steps).enumerate():\n",
    "        if (step) % 1000 == 0:\n",
    "            display.clear_output(wait=True)\n",
    "\n",
    "        if step != 0:\n",
    "            print(f'Time taken for 1000 steps: {time.time()-start:.2f} sec\\n')\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        generate_images(generator, example_input, example_target)\n",
    "        print(f\"Step: {step//1000}k\")\n",
    "\n",
    "    train_step(input_image, target, step)\n",
    "\n",
    "    # Training step\n",
    "    if (step+1) % 10 == 0:\n",
    "        print('.', end='', flush=True)\n",
    "\n",
    "\n",
    "    # Save (checkpoint) the model every 5k steps\n",
    "    if (step + 1) % 5000 == 0:\n",
    "      checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {log_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(train_dataset, test_dataset, steps = 40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {checkpoint_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the trained model on a few examples from the test set\n",
    "for inp, tar in test_dataset.take(10):\n",
    "    generate_images(generator, inp, tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the trained model on a few examples from the test set\n",
    "for inp, tar in test_dataset.take(10):\n",
    "    generate_images(generator, inp, tar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  2. **********************************************************************\n",
    "#        ****************************** Cyclegan*************************** \n",
    "#      **********************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    " \n",
    "import pathlib\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'cityscapes'\n",
    "_URL = f'http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/{dataset_name}.tar.gz'\n",
    "\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    fname=f\"{dataset_name}.tar.gz\",\n",
    "    origin=_URL,\n",
    "    extract=True)\n",
    "\n",
    "path_to_zip  = pathlib.Path(path_to_zip)\n",
    "\n",
    "PATH = path_to_zip.parent/dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(PATH.parent.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(PATH.parent.iterdir())\n",
    "sample_image = tf.io.read_file(str(PATH /'train/1.jpg'))\n",
    "sample_image = tf.io.decode_jpeg(sample_image, channels=3)\n",
    "print(sample_image.shape)\n",
    "plt.figure()\n",
    "plt.imshow(sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE_TRAIN = 400\n",
    "BUFFER_SIZE_TEST  = 6287\n",
    "\n",
    "BATCH_SIZE  = 1\n",
    "\n",
    "IMG_WIDTH   = 256\n",
    "IMG_HEIGHT  = 256\n",
    "\n",
    "R_IMG_WIDTH   = 286\n",
    "R_IMG_HEIGHT  = 286\n",
    "\n",
    "PATH      = 'cityscapes'\n",
    "train_dir = 'cityscapes' + '/train'\n",
    "test_dir  = 'cityscapes' + '/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para cargar las imágenes\n",
    "def load (image_file, bool_img = False):\n",
    "    # Read and decode an image file to a uint8 tensor\n",
    "    image = tf.io.read_file (image_file)\n",
    "    image = tf.io.decode_jpeg(image, channels=3)\n",
    "    \n",
    "    # Split each image tensor into two tensors:\n",
    "    # - one with a real building facade image\n",
    "    # - one with an architecture label image \n",
    "    w = tf.shape (image)[1]\n",
    "    w = w // 2\n",
    "    input_image = image [:, w:, :]\n",
    "    real_image  = image [:, :w, :]\n",
    "    # Convert both images to float32 tensors\n",
    "    input_image = tf.cast (input_image, tf.float32)\n",
    "    real_image  = tf.cast (real_image, tf.float32)\n",
    "    \n",
    "    if bool_img:\n",
    "      # Casting to int for matplotlib to display the images\n",
    "        plt.figure()\n",
    "        plt.imshow(input_image / 255.0)\n",
    "        plt.figure()\n",
    "        plt.imshow(real_image / 255.0)\n",
    "  \n",
    "    return input_image, real_image\n",
    "\n",
    "inp, re = load (str(PATH + '/train/1.jpg'), bool_img = True)\n",
    "\n",
    " \n",
    "\n",
    "# Función para crear el conjunto de datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_jitter (image):\n",
    "    # resizing to 286 x 286 x 3\n",
    "    image = tf.image.resize (image, [R_IMG_HEIGHT, R_IMG_WIDTH],\n",
    "                                      method = 'nearest')\n",
    "    # randomly cropping to 256 x 256 x 3\n",
    "    image = random_crop(image)\n",
    "\n",
    "    # random mirroring\n",
    "    image = tf.image.random_flip_left_right(image) #Modificiamos la imagenpara aumentar el numero de muestras\n",
    "\n",
    "    return image\n",
    "\n",
    "def random_crop(image):\n",
    "    cropped_image = tf.image.random_crop(\n",
    "                      image, size=[IMG_HEIGHT, IMG_WIDTH, 3])\n",
    "\n",
    "    return cropped_image\n",
    "\n",
    "def normalize (image):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image / 127.5) - 1\n",
    "    return image\n",
    "\n",
    "\n",
    "def resize (input_image, real_image, height, width):\n",
    "    input_image = tf.image.resize(input_image, [height, width],\n",
    "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    real_image = tf.image.resize(real_image, [height, width],\n",
    "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_train (image_file):\n",
    "    input_image, real_image = load (image_file)\n",
    "    \n",
    "    input_image  = random_jitter (input_image)\n",
    "    real_image   = random_jitter (real_image)\n",
    "    \n",
    "    input_image  = normalize (input_image)\n",
    "    real_image   = normalize (real_image)\n",
    " \n",
    "    \n",
    "    return input_image, real_image\n",
    "\n",
    "def load_image_test (image_file):\n",
    "    input_image, real_image = load (image_file)\n",
    " \n",
    "    #input_image, real_image = resize (input_image, real_image,IMG_HEIGHT, IMG_WIDTH)\n",
    "    \n",
    "    input_image = normalize (input_image)\n",
    "    real_image  = normalize (real_image)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Directorios de entrenamiento y etiquetas de entrenamiento\n",
    "train_dir  = PATH + '/train'\n",
    "test_dir   = PATH +  '/val'\n",
    "\n",
    "# Obtener rutas de todas las imágenes de entrenamiento\n",
    "train_image_paths = [os.path.join(train_dir, fname) for fname in os.listdir (train_dir)]\n",
    "test_image_paths  = [os.path.join(test_dir, fname) for fname in os.listdir (test_dir)]\n",
    "\n",
    "\n",
    "# Crear dataset de tensorflow a partir de las rutas de las imágenes\n",
    " # tf.data.Dataset (imgA),tf.data.Dataset (imgB) = \n",
    "lstA = []\n",
    "lstB = []\n",
    "for f in train_image_paths:\n",
    "    imgA, imgB = load_image_train (f)\n",
    "    lstA.append (imgA)\n",
    "    lstB.append (imgB)\n",
    "\n",
    "#convertir la lista de imágenes y etiquetas a tensores\n",
    "dtsA = tf.convert_to_tensor(lstA)\n",
    "dtsB = tf.convert_to_tensor(lstB)\n",
    "\n",
    "train_datasetA = tf.data.Dataset.from_tensor_slices(dtsA).batch (BATCH_SIZE)\n",
    "train_datasetB = tf.data.Dataset.from_tensor_slices(dtsB).batch (BATCH_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "# Crear dataset de tensorflow a partir de las rutas de las imágenes\n",
    " # tf.data.Dataset (imgA),tf.data.Dataset (imgB) = \n",
    "lstA = []\n",
    "lstB = []\n",
    "for f in test_image_paths:\n",
    "    imgA, imgB = load_image_test(f)\n",
    "    lstA.append(imgA)\n",
    "    lstB.append(imgB)\n",
    "\n",
    "#convertir la lista de imágenes y etiquetas a tensores\n",
    "dtsA = tf.convert_to_tensor(lstA)\n",
    "dtsB = tf.convert_to_tensor(lstB)\n",
    "\n",
    "test_datasetA = tf.data.Dataset.from_tensor_slices(dtsA).batch (BATCH_SIZE)\n",
    "test_datasetB = tf.data.Dataset.from_tensor_slices(dtsB).batch (BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dtsA = next(iter(train_datasetA))\n",
    "sample_dtsB = next(iter(train_datasetB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Obtener un iterador para las primeras 3 imágenes\n",
    "subset_iter = iter(train_datasetA.take(5))\n",
    "\n",
    "fig, axs = plt.subplots(1, 5, figsize=(10, 10))\n",
    "for i, img in enumerate(subset_iter):\n",
    "    img = (img + 1) / 2.0  # Escalar los valores de los píxeles al rango [0, 1]\n",
    "    axs[i].imshow(img[0])\n",
    "plt.show()\n",
    "\n",
    " \n",
    "\n",
    "# Obtener un iterador para las primeras 3 imágenes\n",
    "subset_iter = iter(train_datasetB.take(5))\n",
    "\n",
    "fig, axs = plt.subplots(1, 5, figsize=(10, 10))\n",
    "for i, img in enumerate(subset_iter):\n",
    "    img = (img + 1) / 2.0  # Escalar los valores de los píxeles al rango [0, 1]\n",
    "    axs[i].imshow(img[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Obtener un iterador para las primeras 3 imágenes\n",
    "subset_iter = iter(test_datasetA.take(5))\n",
    "\n",
    "fig, axs = plt.subplots(1, 5, figsize=(10, 10))\n",
    "for i, img in enumerate(subset_iter):\n",
    "    img = (img + 1) / 2.0  # Escalar los valores de los píxeles al rango [0, 1]\n",
    "    axs[i].imshow(img[0])\n",
    "plt.show()\n",
    "\n",
    " \n",
    "\n",
    "# Obtener un iterador para las primeras 3 imágenes\n",
    "subset_iter = iter(test_datasetB.take(5))\n",
    "\n",
    "fig, axs = plt.subplots(1, 5, figsize=(10, 10))\n",
    "for i, img in enumerate(subset_iter):\n",
    "    img = (img + 1) / 2.0  # Escalar los valores de los píxeles al rango [0, 1]\n",
    "    axs[i].imshow(img[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CycleGan Model:  \n",
    "\n",
    " github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py) via the installed [tensorflow_examples](https://github.com/tensorflow/examples) package.\n",
    "\n",
    "The model architecture used in this tutorial is very similar to what was used in [pix2pix](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py). Some of the differences are:\n",
    "\n",
    "* Cyclegan \n",
    "There are 2 generators (G and F) and 2 discriminators (X and Y) being trained here. \n",
    "\n",
    "* Generator `G` learns to transform image `X` to image `Y`. $(G: X -> Y)$\n",
    "* Generator `F` learns to transform image `Y` to image `X`. $(F: Y -> X)$\n",
    "* Discriminator `D_X` learns to differentiate between image `X` and generated image `X` (`F(Y)`).\n",
    "* Discriminator `D_Y` learns to differentiate between image `Y` and generated image `Y` (`G(X)`).\n",
    "\n",
    "- El modelo toma una entrada de forma (256, 256, 3) mediante una capa de entrada.\n",
    "- A continuación, se aplican varias capas convolucionales con función de activación LeakyReLU y Batch Normalization para realizar la codificación (o downsampling) de la entrada.\n",
    "- Luego, se aplican varias capas de convolución transpuesta con función de activación ReLU y Batch Normalization para realizar la decodificación (o upsampling) de la entrada codificada.\n",
    "- Se utilizan capas de dropout para regularizar el modelo y prevenir el sobreajuste.\n",
    "- Se utilizan capas de concatenación para fusionar las características de las capas de convolución transpuesta con las capas de convolución correspondientes durante la codificación.\n",
    "- La capa de salida tiene una activación 'tanh' para generar valores de píxeles en el rango [-1, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 3\n",
    "\n",
    "generator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n",
    "generator_f = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n",
    "\n",
    "discriminator_x = pix2pix.discriminator(norm_type='instancenorm', target=False)\n",
    "discriminator_y = pix2pix.discriminator(norm_type='instancenorm', target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_dtsA = generator_g (sample_dtsA )\n",
    "to_dtsB = generator_f (sample_dtsB)\n",
    "plt.figure(figsize=(8, 8))\n",
    "contrast = 8\n",
    "\n",
    "imgs = [sample_dtsA, to_dtsB, sample_dtsB, to_dtsA]\n",
    "title = ['dtsA', 'To dtsB', 'dtsB', 'To dtsA']\n",
    "\n",
    "for i in range(len(imgs)):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.title(title[i])\n",
    "    if i % 2 == 0:\n",
    "        plt.imshow(imgs[i][0] * 0.5 + 0.5)\n",
    "    else:\n",
    "        plt.imshow(imgs[i][0] * 0.5 * contrast + 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title('Is a real zebra?')\n",
    "plt.imshow(discriminator_y(sample_dtsB)[0, ..., -1], cmap='RdBu_r')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Is a real horse?')\n",
    "plt.imshow(discriminator_x(sample_dtsA)[0, ..., -1], cmap='RdBu_r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real, generated):\n",
    "    real_loss = loss_obj(tf.ones_like(real), real)\n",
    "\n",
    "    generated_loss = loss_obj(tf.zeros_like(generated), generated)\n",
    "\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "    return total_disc_loss * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(generated):\n",
    "    return loss_obj(tf.ones_like(generated), generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizador\n",
    "\n",
    "#### Cycle loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cycle_loss(real_image, cycled_image):\n",
    "    loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
    "\n",
    "    return LAMBDA * loss1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_loss(real_image, same_image):\n",
    "    loss = tf.reduce_mean(tf.abs(real_image - same_image))\n",
    "    return LAMBDA * 0.5 * loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the optimizers for all the generators and the discriminators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_g_optimizer = tf.keras.optimizers.Adam (2e-4, beta_1 = 0.5)\n",
    "generator_f_optimizer = tf.keras.optimizers.Adam (2e-4, beta_1 = 0.5)\n",
    "\n",
    "discriminator_x_optimizer = tf.keras.optimizers.Adam (2e-4, beta_1 = 0.5)\n",
    "discriminator_y_optimizer = tf.keras.optimizers.Adam (2e-4, beta_1 = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/cityscapes/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(generator_g=generator_g,\n",
    "                           generator_f=generator_f,\n",
    "                           discriminator_x=discriminator_x,\n",
    "                           discriminator_y=discriminator_y,\n",
    "                           generator_g_optimizer=generator_g_optimizer,\n",
    "                           generator_f_optimizer=generator_f_optimizer,\n",
    "                           discriminator_x_optimizer=discriminator_x_optimizer,\n",
    "                           discriminator_y_optimizer=discriminator_y_optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(model, test_input):\n",
    "    prediction = model(test_input)\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "\n",
    "    display_list = [test_input[0], prediction[0]]\n",
    "    title = ['Input Image', 'Predicted Image']\n",
    "\n",
    "    for i in range(2):\n",
    "        plt.subplot(1, 2, i+1)\n",
    "        plt.title(title[i])\n",
    "        # getting the pixel values between [0, 1] to plot it.\n",
    "        plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(real_x, real_y):\n",
    "  # persistent is set to True because the tape is used more than\n",
    "  # once to calculate the gradients.\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # Generator G translates X -> Y\n",
    "        # Generator F translates Y -> X.\n",
    "\n",
    "        fake_y = generator_g(real_x, training=True)\n",
    "        cycled_x = generator_f(fake_y, training=True)\n",
    "\n",
    "        fake_x = generator_f(real_y, training=True)\n",
    "        cycled_y = generator_g(fake_x, training=True)\n",
    "\n",
    "        # same_x and same_y are used for identity loss.\n",
    "        same_x = generator_f(real_x, training=True)\n",
    "        same_y = generator_g(real_y, training=True)\n",
    "\n",
    "        disc_real_x = discriminator_x(real_x, training=True)\n",
    "        disc_real_y = discriminator_y(real_y, training=True)\n",
    "\n",
    "        disc_fake_x = discriminator_x(fake_x, training=True)\n",
    "        disc_fake_y = discriminator_y(fake_y, training=True)\n",
    "\n",
    "        # calculate the loss\n",
    "        gen_g_loss = generator_loss(disc_fake_y)\n",
    "        gen_f_loss = generator_loss(disc_fake_x)\n",
    "\n",
    "        total_cycle_loss = calc_cycle_loss(real_x, cycled_x) + calc_cycle_loss(real_y, cycled_y)\n",
    "\n",
    "        # Total generator loss = adversarial loss + cycle loss\n",
    "        total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_loss(real_y, same_y)\n",
    "        total_gen_f_loss = gen_f_loss + total_cycle_loss + identity_loss(real_x, same_x)\n",
    "\n",
    "        disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)\n",
    "        disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n",
    "  \n",
    "    # Calculate the gradients for generator and discriminator\n",
    "    generator_g_gradients = tape.gradient(total_gen_g_loss, \n",
    "                                        generator_g.trainable_variables)\n",
    "    generator_f_gradients = tape.gradient(total_gen_f_loss, \n",
    "                                        generator_f.trainable_variables)\n",
    "\n",
    "    discriminator_x_gradients = tape.gradient(disc_x_loss, \n",
    "                                            discriminator_x.trainable_variables)\n",
    "    discriminator_y_gradients = tape.gradient(disc_y_loss, \n",
    "                                            discriminator_y.trainable_variables)\n",
    "  \n",
    "    # Apply the gradients to the optimizer\n",
    "    generator_g_optimizer.apply_gradients(zip(generator_g_gradients, \n",
    "                                            generator_g.trainable_variables))\n",
    "\n",
    "    generator_f_optimizer.apply_gradients(zip(generator_f_gradients, \n",
    "                                            generator_f.trainable_variables))\n",
    "\n",
    "    discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients,\n",
    "                                                discriminator_x.trainable_variables))\n",
    "\n",
    "    discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients,\n",
    "                                                discriminator_y.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    n = 0\n",
    "    for image_x, image_y in tf.data.Dataset.zip((train_datasetA, train_datasetB)):\n",
    "        train_step(image_x, image_y)\n",
    "        if n % 10 == 0:\n",
    "            print ('.', end='')\n",
    "        n += 1\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    # Using a consistent image (sample_horse) so that the progress of the model\n",
    "    # is clearly visible.\n",
    "    generate_images(generator_g, sample_dtsA)\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                             ckpt_save_path))\n",
    "\n",
    "    print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n",
    "                                                      time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate using test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the trained model on the test dataset\n",
    "for inp in train_datasetA.take(25):\n",
    "    generate_images(generator_g, inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inp in train_datasetB.take(25):\n",
    "    generate_images(generator_f, inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the trained model on the test dataset\n",
    "for inp in test_datasetA.take(25):\n",
    "    generate_images(generator_g, inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the trained model on the test dataset\n",
    "for inp in test_datasetB.take(25):\n",
    "    generate_images(generator_f, inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONCLUSIÓN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "anto Pix2Pix como CycleGAN son algoritmos muy populares y efectivos para la tarea de traducción de imágenes. En la segmentación de calles, ambos pueden ser útiles y dar buenos resultados dependiendo de la configuración y entrenamiento del modelo.\n",
    "\n",
    "Pix2Pix se enfoca en aprender una función de mapeo directo entre dos dominios de imagen, lo que significa que necesita un conjunto de datos emparejados de imágenes de entrada y salida para el entrenamiento. Por otro lado, CycleGAN es capaz de aprender la relación entre dos dominios de imagen sin la necesidad de imágenes emparejadas, lo que lo hace más flexible en términos de entrenamiento.\n",
    "\n",
    "En nuestro caso, el modelo px2pix para Cityscapes parece que da mejores resultados con menos epochs de entrenamieto que el modelo cyclegan (DCGSN). Pero como hemos comentado, ambis modelos son optimos para la segmentación de calles y detección de objetos en la circulación de coches autnomos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "P3_parte_2_GANs_condicionales.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
