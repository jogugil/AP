{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2Ubr-O3iQVy"
   },
   "source": [
    "# **Práctica 3**: GANs\n",
    "# Parte 2: GANs simples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALUMNO: JOSÉ JAVIER GUTIÉRREZ GIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ai_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ai_benchmark import AIBenchmark\n",
    "#benchmark_gpu = AIBenchmark(use_CPU=False)\n",
    "#benchmark_gpu.run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "if tf.config.list_physical_devices('GPU'): \n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    print('(*) gpus List: ',gpus)\n",
    "    device_name = tf.test.gpu_device_name()\n",
    "else:\n",
    "    device_name = '/CPU:0'\n",
    "print('device_name:',device_name)\n",
    "\n",
    "\n",
    "# Limitación la memoria de la GPU\n",
    "#config = tf.compat.v1.ConfigProto (allow_soft_placement=True)\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.6\n",
    "#tf.compat.v1.keras.backend.set_session (tf.compat.v1.Session(config=config))\n",
    "\n",
    "# Permitir crecimiento de la memoria\n",
    " \n",
    "# try:\n",
    "\n",
    "    # if gpus:\n",
    "      # Establecer el crecimiento de memoria en True para permitir asignación dinámica de memoria\n",
    "      # for gpu in gpus:\n",
    "        # print (' -:',gpu)\n",
    "        # tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        # Inicializa el dispositivo de la GPU\n",
    "        # tf.config.experimental.set_logical_device_configuration(gpu, [tf.config.LogicalDeviceConfiguration(memory_limit = 2000)])\n",
    "\n",
    "# except Exception as e:\n",
    "    # Captura la excepción y muestra información sobre el error\n",
    "    # print(\"Se produjo una excepción:\", e)\n",
    "   #  print('(*) Invalid device or cannot modify virtual devices once initialized.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "libs = [\"protobuf\",\"tfds-nightly\",\"googleapis-common-protos\", \"mediapipe\",\n",
    "        \"tb-nightly\", \"tensorboard\", \"tensorflow\", \"tensorflow-gpu\",\"tensorflow-intel\", \"tensorflow-metadata\", \n",
    "        \"tf_nightly_intel\"]\n",
    "\n",
    "[lb+' = '+pkg_resources.get_distribution(lb).version for lb in libs] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DVKZamBiY1l"
   },
   "source": [
    "### Ejercicio prelab:\n",
    "Corre el código de pix2pix de la página de tensorflow:\n",
    "https://www.tensorflow.org/tutorials/generative/pix2pix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "330dWmlel6sV"
   },
   "source": [
    "### · Ejercicio 1:\n",
    "Corre el código de pix2pix de la página de tensorflow (el anterior) para el dataset \"maps\". Testealo pasándole como entrada una imagen del mapa de google maps de la ETSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Se ha descargado el notebook pix2pix_facades.ipynb y ejecutado para estudiar el código. \n",
    "- Posteriormente se ha introduido el codigo y modificado en este notebook para que se ejecute con el dataset \"map\". Se guarda en el notebook **'pix2pix_map.ipynb'**. Las modificaciones realizadas han sido: \n",
    "\n",
    "    * Modificamps la variable que guarda el nombre del dataset 'dataset_name = \"maps\"'  y se descarga el tar.gz de su url '_URL = f'http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/{dataset_name}.tar.gz''\n",
    "    * El dataset  facades tenía imagenes de 256x512 ( 2 imagenes de 256x256). En este caso, el dataset map tiene dos imagenes de 600x600. Por lo que modificamos el código para que pueda trabajar con estos tamaños.\n",
    "        * Hacemos quun resize a 6030x3630 y luego un crop aleatorio para eliminar de forma aleatoria pixels tal que se queden en imagenes de 256x256. \n",
    "        \n",
    "De esta forma se puede ejecutar el código del modelo sin ninguna modificación más. Esto se debe hacer ya que el algoritmo pix2pix obliga a tener el mismo tamaño de imagen de entrada y salida. Otra opción sería utilizar el algoritmo cycleGan en el cual no es necesario que se tenga el mismo tamaño de imagen.\n",
    "\n",
    "Nota: El algoritmo pix2pix utlita una arquitectura U-NET para el generado el cual tiene un codificador decodificador. La arquitectura U-Net fue presentada por Olaf Ronneberger, Philipp Fischer, Thomas Brox en 2015 para la detección de tumores, pero desde entonces se ha descubierto que es útil en múltiples industrias. Como herramienta de segmentación de imágenes, el modelo tiene como objetivo clasificar cada píxel como una de las clases de salida. \n",
    "\n",
    "Y el algoritmo PatchGAN  para el decodificador, el cual es una variante del discriminador utilizado en GAN. A diferencia del discriminador convencional, que evalúa la imagen completa, PatchGAN evalúa la imagen en pequeñas regiones llamadas parches. Esto permite que el discriminador tenga más información detallada sobre la imagen y pueda capturar características finas y de alta frecuencia.\n",
    "\n",
    "En resumen, GAN es un marco de red neuronal generativa que consta de dos partes, mientras que PatchGAN es una variante del discriminador utilizado en GAN que evalúa la imagen en parches en lugar de la imagen completa.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código pix2pix_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"maps\" #@param [\"cityscapes\", \"edges2handbags\", \"edges2shoes\", \"facades\", \"maps\", \"night2day\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_URL = f'http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/{dataset_name}.tar.gz'\n",
    "\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    fname=f\"{dataset_name}.tar.gz\",\n",
    "    origin=_URL,\n",
    "    extract=True)\n",
    "\n",
    "path_to_zip  = pathlib.Path(path_to_zip)\n",
    "\n",
    "PATH = path_to_zip.parent/dataset_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(PATH.parent.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = tf.io.read_file(str(PATH / 'train/1.jpg'))\n",
    "sample_image = tf.io.decode_jpeg(sample_image)\n",
    "print(sample_image.shape)\n",
    "plt.figure()\n",
    "plt.imshow(sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(image_file):\n",
    "    # Read and decode an image file to a uint8 tensor\n",
    "    image = tf.io.read_file(image_file)\n",
    "    image = tf.io.decode_jpeg(image)\n",
    "\n",
    "    # Split each image tensor into two tensors:\n",
    "    # - one with a real building facade image\n",
    "    # - one with an architecture label image \n",
    "    w = tf.shape(image)[1]\n",
    "    w = w // 2\n",
    "    input_image = image[:, w:, :]\n",
    "    real_image  = image[:, :w, :]\n",
    "\n",
    "    # Convert both images to float32 tensors\n",
    "    input_image = tf.cast(input_image, tf.float32)\n",
    "    real_image  = tf.cast(real_image, tf.float32)\n",
    "\n",
    "    return input_image, real_image\n",
    "\n",
    "inp, re = load(str(PATH / 'train/100.jpg'))\n",
    "# Casting to int for matplotlib to display the images\n",
    "plt.figure()\n",
    "plt.imshow(inp / 255.0)\n",
    "plt.figure()\n",
    "plt.imshow(re / 255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The facade training set consist of 400 images\n",
    "BUFFER_SIZE = 400\n",
    "# The batch size of 1 produced better results for the U-Net in the original pix2pix experiment\n",
    "BATCH_SIZE  = 1\n",
    "# Each image is 256x256 in size\n",
    "IMG_WIDTH   = 256\n",
    "IMG_HEIGHT  = 256\n",
    "\n",
    "# resize\n",
    "RE_IMG_WIDTH   = 630 # --123--\n",
    "RE_IMG_HEIGHT  = 630 # --123--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(input_image, real_image, height, width):\n",
    "    input_image = tf.image.resize(input_image, [height, width],\n",
    "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    real_image = tf.image.resize(real_image, [height, width],\n",
    "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(input_image, real_image):\n",
    "    stacked_image = tf.stack([input_image, real_image], axis=0)\n",
    "    cropped_image = tf.image.random_crop(\n",
    "                                  stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
    "\n",
    "    return cropped_image[0], cropped_image[1]\n",
    "\n",
    "# Normalizing the images to [-1, 1]\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the images to [-1, 1]\n",
    "def normalize(input_image, real_image):\n",
    "    input_image = (input_image / 127.5) - 1\n",
    "    real_image = (real_image / 127.5) - 1\n",
    "\n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def random_jitter(input_image, real_image):\n",
    "    # Resizing to 286x286\n",
    "    input_image, real_image = resize(input_image, real_image, RE_IMG_WIDTH, RE_IMG_HEIGHT)\n",
    "\n",
    "    # Random cropping back to 256x256\n",
    "    input_image, real_image = random_crop(input_image, real_image)\n",
    "\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        # Random mirroring\n",
    "        input_image = tf.image.flip_left_right(input_image)\n",
    "        real_image = tf.image.flip_left_right(real_image)\n",
    "\n",
    "    return input_image, real_image\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "for i in range(4):\n",
    "    rj_inp, rj_re = random_jitter(inp, re)\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.imshow(rj_inp / 255.0)\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    rj_inp, rj_re = random_jitter(inp, re)\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.imshow(rj_re / 255.0)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_train(image_file):\n",
    "    input_image, real_image = load(image_file)\n",
    "    input_image, real_image = random_jitter(input_image, real_image)\n",
    "    input_image, real_image = normalize(input_image, real_image)\n",
    "\n",
    "    return input_image, real_image\n",
    "\n",
    "def load_image_test(image_file):\n",
    "    input_image, real_image = load(image_file)\n",
    "    input_image, real_image = resize(input_image, real_image,\n",
    "                                   IMG_HEIGHT, IMG_WIDTH)\n",
    "    input_image, real_image = normalize(input_image, real_image)\n",
    "\n",
    "    return input_image, real_image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.list_files(str(PATH / 'train/*.jpg'))\n",
    "train_dataset = train_dataset.map(load_image_train,\n",
    "                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "try:\n",
    "    test_dataset = tf.data.Dataset.list_files(str(PATH / 'test/*.jpg'))\n",
    "except tf.errors.InvalidArgumentError:\n",
    "    test_dataset = tf.data.Dataset.list_files(str(PATH / 'val/*.jpg'))\n",
    "test_dataset = test_dataset.map(load_image_test)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "          tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                                 kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "    if apply_batchnorm:\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    result.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    return result\n",
    "\n",
    "down_model = downsample(3, 4)\n",
    "down_result = down_model(tf.expand_dims(inp, 0))\n",
    "print (down_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(filters, size, apply_dropout=False):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "                tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                                    padding='same',\n",
    "                                                    kernel_initializer=initializer,\n",
    "                                                    use_bias=False))\n",
    "\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    if apply_dropout:\n",
    "        result.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "    result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "    return result\n",
    "\n",
    "up_model  = upsample(3, 4)\n",
    "up_result = up_model(down_result)\n",
    "print (up_result.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator():\n",
    "    inputs = tf.keras.layers.Input(shape=[256, 256, 3])\n",
    "\n",
    "    down_stack = [\n",
    "        downsample(64, 4, apply_batchnorm=False),  # (batch_size, 128, 128, 64)\n",
    "        downsample(128, 4),  # (batch_size, 64, 64, 128)\n",
    "        downsample(256, 4),  # (batch_size, 32, 32, 256)\n",
    "        downsample(512, 4),  # (batch_size, 16, 16, 512)\n",
    "        downsample(512, 4),  # (batch_size, 8, 8, 512)\n",
    "        downsample(512, 4),  # (batch_size, 4, 4, 512)\n",
    "        downsample(512, 4),  # (batch_size, 2, 2, 512)\n",
    "        downsample(512, 4),  # (batch_size, 1, 1, 512)\n",
    "    ]\n",
    "\n",
    "    up_stack = [\n",
    "        upsample(512, 4, apply_dropout=True),  # (batch_size, 2, 2, 1024)\n",
    "        upsample(512, 4, apply_dropout=True),  # (batch_size, 4, 4, 1024)\n",
    "        upsample(512, 4, apply_dropout=True),  # (batch_size, 8, 8, 1024)\n",
    "        upsample(512, 4),  # (batch_size, 16, 16, 1024)\n",
    "        upsample(256, 4),  # (batch_size, 32, 32, 512)\n",
    "        upsample(128, 4),  # (batch_size, 64, 64, 256)\n",
    "        upsample(64, 4),  # (batch_size, 128, 128, 128)\n",
    "    ]\n",
    "\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
    "                                         strides=2,\n",
    "                                         padding='same',\n",
    "                                         kernel_initializer=initializer,\n",
    "                                         activation='tanh')  # (batch_size, 256, 256, 3)\n",
    "\n",
    "    x = inputs\n",
    "\n",
    "    # Downsampling through the model\n",
    "    skips = []\n",
    "    for down in down_stack:\n",
    "        x = down(x)\n",
    "        skips.append(x)\n",
    "\n",
    "    skips = reversed(skips[:-1])\n",
    "\n",
    "    # Upsampling and establishing the skip connections\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        x = tf.keras.layers.Concatenate()([x, skip])\n",
    "\n",
    "    x = last(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "generator = Generator()\n",
    "tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rj_inp, rj_re = random_jitter(inp, re)  # --123--  \n",
    "print(rj_inp.shape)\n",
    "gen_output = generator(rj_inp[tf.newaxis, ...], training=False) # --123--  \n",
    "plt.imshow(gen_output[0, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(disc_generated_output, gen_output, target):\n",
    "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "    # Mean absolute error\n",
    "    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "\n",
    "    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
    "\n",
    "    return total_gen_loss, gan_loss, l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Discriminator():\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')\n",
    "    tar = tf.keras.layers.Input(shape=[256, 256, 3], name='target_image')\n",
    "\n",
    "    x = tf.keras.layers.concatenate([inp, tar])  # (batch_size, 256, 256, channels*2)\n",
    "\n",
    "    down1 = downsample(64, 4, False)(x)  # (batch_size, 128, 128, 64)\n",
    "    down2 = downsample(128, 4)(down1)  # (batch_size, 64, 64, 128)\n",
    "    down3 = downsample(256, 4)(down2)  # (batch_size, 32, 32, 256)\n",
    "\n",
    "    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)  # (batch_size, 34, 34, 256)\n",
    "    conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
    "                                kernel_initializer=initializer,\n",
    "                                use_bias=False)(zero_pad1)  # (batch_size, 31, 31, 512)\n",
    "\n",
    "    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
    "\n",
    "    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
    "\n",
    "    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (batch_size, 33, 33, 512)\n",
    "\n",
    "    last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
    "                                kernel_initializer=initializer)(zero_pad2)  # (batch_size, 30, 30, 1)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inp, tar], outputs=last)\n",
    "\n",
    "discriminator = Discriminator()\n",
    "tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rj_inp, rj_re = random_jitter(inp, re)  # --123--   \n",
    "disc_out = discriminator([rj_inp[tf.newaxis, ...], gen_output], training=False)\n",
    "plt.imshow(disc_out[0, ..., -1], vmin=-20, vmax=20, cmap='RdBu_r')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
    "\n",
    "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "    return total_disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer     = tf.keras.optimizers.Adam (2e-4, beta_1 = 0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam (2e-4, beta_1 = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir    = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(model, test_input, tar):\n",
    "    prediction = model(test_input, training=True)\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    display_list = [test_input[0], tar[0], prediction[0]]\n",
    "    title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
    "\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        plt.title(title[i])\n",
    "        # Getting the pixel values in the [0, 1] range to plot.\n",
    "        plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "for example_input, example_target in test_dataset.take(1):\n",
    "    generate_images(generator, example_input, example_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir=\"logs/\"\n",
    "\n",
    "summary_writer = tf.summary.create_file_writer(\n",
    "  log_dir + \"fit_maps/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(input_image, target, step):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        gen_output = generator(input_image, training=True)\n",
    "\n",
    "        disc_real_output = discriminator([input_image, target], training=True)\n",
    "        disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
    "\n",
    "        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n",
    "        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
    "\n",
    "    generator_gradients = gen_tape.gradient(gen_total_loss,\n",
    "                                          generator.trainable_variables)\n",
    "    discriminator_gradients = disc_tape.gradient(disc_loss,\n",
    "                                               discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(generator_gradients,\n",
    "                                          generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
    "                                              discriminator.trainable_variables))\n",
    "\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('gen_total_loss', gen_total_loss, step=step//1000)\n",
    "        tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=step//1000)\n",
    "        tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=step//1000)\n",
    "        tf.summary.scalar('disc_loss', disc_loss, step=step//1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(train_ds, test_ds, steps):\n",
    "    example_input, example_target = next(iter(test_ds.take(1)))\n",
    "    start = time.time()\n",
    "\n",
    "    for step, (input_image, target) in train_ds.repeat().take(steps).enumerate():\n",
    "        if (step) % 1000 == 0:\n",
    "            display.clear_output(wait=True)\n",
    "\n",
    "            if step != 0:\n",
    "                print(f'Time taken for 1000 steps: {time.time()-start:.2f} sec\\n')\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            generate_images(generator, example_input, example_target)\n",
    "            print(f\"Step: {step//1000}k\")\n",
    "\n",
    "        train_step(input_image, target, step)\n",
    "\n",
    "        # Training step\n",
    "        if (step+1) % 10 == 0:\n",
    "            print('.', end='', flush=True)\n",
    "\n",
    "\n",
    "        # Save (checkpoint) the model every 5k steps\n",
    "        if (step + 1) % 5000 == 0:\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {log_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {log_dir}\n",
    "fit(train_dataset, test_dataset, steps=40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {checkpoint_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the trained model on a few examples from the test set\n",
    "for inp, tar in test_dataset.take(10):\n",
    "    generate_images(generator, inp, tar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilización imagen ETSE - GoogleMaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "img_etse_c = \"etse_c.jpg\"\n",
    "img_etse_s = \"etse_s.jpg\"\n",
    "# Abre la imagen en formato JPEG\n",
    "\n",
    "img_c = tf.io.read_file(img_etse_c)\n",
    "img_c = tf.io.decode_jpeg(img_c)\n",
    "\n",
    "img_s = tf.io.read_file(img_etse_s)\n",
    "img_s = tf.io.decode_jpeg(img_s)\n",
    "\n",
    "# Cast type to float32\n",
    "img_c = tf.cast(img_c, tf.float32)\n",
    "img_s = tf.cast(img_s, tf.float32)\n",
    "\n",
    "# Muestra la imagen\n",
    "plt.imshow(img_c/255.)\n",
    "plt.show()\n",
    " \n",
    "plt.imshow(img_s/255.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image, real_image = resize(img_c, img_s,\n",
    "                                   IMG_HEIGHT, IMG_WIDTH)\n",
    "\n",
    "print\n",
    "inp, tar = normalize(input_image, real_image)\n",
    "\n",
    "inp = np.expand_dims(inp, axis=0)\n",
    "tar = np.expand_dims(tar, axis=0)\n",
    "\n",
    "plt.imshow(inp[0,:,:,:])\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(tar[0,:,:,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Testeamos la arquitectura pix2pix entrenada con el dataset maps con el mapa de la ETSE esxtraido de googlemaps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_images(generator, inp, tar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2w_a3ZzmtbR"
   },
   "source": [
    "### Ejercicio prelab:\n",
    "Corre el código de cyclegan de la página de tensorflow:\n",
    "https://www.tensorflow.org/tutorials/generative/cyclegan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0xJLXjam1qw"
   },
   "source": [
    "### · Ejercicio 2:\n",
    "Modifica el código de cyclegan de la página de tensorflow (el anterior) para que funcione sobre el dataset de documentos sucios que os proporcionamos. Comprueba que los modelos han aprendido a \"limpiar\" y a \"ensuciar\" documentos.\n",
    "\n",
    "\n",
    "Para modificar el código de CycleGAN de TensorFlow para funcionar en el conjunto de datos de documentos sucios proporcionado, necesitamos ajustar los siguientes parámetros:\n",
    "\n",
    "- BUFFER_SIZE: la cantidad de elementos a mezclar en el conjunto de datos.\n",
    "\n",
    "- BATCH_SIZE: el tamaño del lote para el entrenamiento.\n",
    "\n",
    "- IMG_WIDTH: el ancho de las imágenes en el conjunto de datos.\n",
    "\n",
    "- IMG_HEIGHT: la altura de las imágenes en el conjunto de datos.\n",
    "\n",
    "- OUTPUT_CHANNELS: el número de canales de salida para las imágenes generadas.\n",
    "\n",
    "- dataset: la ubicación del conjunto de datos de documentos sucios.\n",
    "\n",
    "- gan_model: el modelo GAN que se utilizará.\n",
    "\n",
    "Además, también necesitamos ajustar las funciones de carga de imagen para que carguen y preprocesen imágenes del conjunto de datos de documentos sucios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/tensorflow/examples.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, metadata = tfds.load('cycle_gan/horse2zebra',\n",
    "                              with_info=True, as_supervised=True)\n",
    "\n",
    "train_horses, train_zebras = dataset['trainA'], dataset['trainB']\n",
    "test_horses, test_zebras   = dataset['testA'], dataset['testB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE  = 1\n",
    "IMG_WIDTH   = 256\n",
    "IMG_HEIGHT  = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(image):\n",
    "    cropped_image = tf.image.random_crop(\n",
    "      image, size=[IMG_HEIGHT, IMG_WIDTH, 3])\n",
    "\n",
    "    return cropped_image\n",
    "\n",
    "# normalizing the images to [-1, 1]\n",
    "def normalize(image):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image / 127.5) - 1\n",
    "    return image\n",
    "\n",
    "def random_jitter(image):\n",
    "    # resizing to 286 x 286 x 3\n",
    "    image = tf.image.resize(image, [286, 286],\n",
    "                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "    # randomly cropping to 256 x 256 x 3\n",
    "    image = random_crop(image)\n",
    "\n",
    "    # random mirroring\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "    return image\n",
    "\n",
    "def preprocess_image_train(image, label):\n",
    "    image = random_jitter(image)\n",
    "    image = normalize(image)\n",
    "    return image\n",
    "\n",
    "def preprocess_image_test(image, label):\n",
    "    image = normalize(image)\n",
    "    return image\n",
    "\n",
    "train_horses = train_horses.cache().map(\n",
    "    preprocess_image_train, num_parallel_calls=AUTOTUNE).shuffle(\n",
    "    BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "train_zebras = train_zebras.cache().map(\n",
    "    preprocess_image_train, num_parallel_calls=AUTOTUNE).shuffle(\n",
    "    BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "test_horses = test_horses.map(\n",
    "    preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(\n",
    "    BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "test_zebras = test_zebras.map(\n",
    "    preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(\n",
    "    BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  pix2pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 3\n",
    "\n",
    "generator_g = pix2pix.unet_generator (OUTPUT_CHANNELS, norm_type = 'instancenorm')\n",
    "generator_f = pix2pix.unet_generator (OUTPUT_CHANNELS, norm_type = 'instancenorm')\n",
    "\n",
    "discriminator_x = pix2pix.discriminator (norm_type = 'instancenorm', target=False)\n",
    "discriminator_y = pix2pix.discriminator (norm_type = 'instancenorm', target=False)\n",
    "\n",
    "sample_horse = next(iter(train_horses))\n",
    "sample_zebra = next(iter(train_zebras))\n",
    "\n",
    "to_zebra = generator_g(sample_horse)\n",
    "to_horse = generator_f(sample_zebra)\n",
    "plt.figure(figsize=(8, 8))\n",
    "contrast = 8\n",
    "\n",
    "imgs = [sample_horse, to_zebra, sample_zebra, to_horse]\n",
    "title = ['Horse', 'To Zebra', 'Zebra', 'To Horse']\n",
    "\n",
    "for i in range(len(imgs)):\n",
    "  plt.subplot(2, 2, i+1)\n",
    "  plt.title(title[i])\n",
    "  if i % 2 == 0:\n",
    "    plt.imshow(imgs[i][0] * 0.5 + 0.5)\n",
    "  else:\n",
    "    plt.imshow(imgs[i][0] * 0.5 * contrast + 0.5)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title('Is a real zebra?')\n",
    "plt.imshow(discriminator_y(sample_zebra)[0, ..., -1], cmap='RdBu_r')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Is a real horse?')\n",
    "plt.imshow(discriminator_x(sample_horse)[0, ..., -1], cmap='RdBu_r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 10\n",
    "\n",
    "loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real, generated):\n",
    "  real_loss = loss_obj(tf.ones_like(real), real)\n",
    "\n",
    "  generated_loss = loss_obj(tf.zeros_like(generated), generated)\n",
    "\n",
    "  total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "  return total_disc_loss * 0.5\n",
    "\n",
    "def generator_loss(generated):\n",
    "  return loss_obj(tf.ones_like(generated), generated)\n",
    "\n",
    "def calc_cycle_loss(real_image, cycled_image):\n",
    "  loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
    "  \n",
    "  return LAMBDA * loss1\n",
    "\n",
    "def identity_loss(real_image, same_image):\n",
    "  loss = tf.reduce_mean(tf.abs(real_image - same_image))\n",
    "  return LAMBDA * 0.5 * loss\n",
    "\n",
    "generator_g_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "generator_f_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "discriminator_x_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(generator_g=generator_g,\n",
    "                           generator_f=generator_f,\n",
    "                           discriminator_x=discriminator_x,\n",
    "                           discriminator_y=discriminator_y,\n",
    "                           generator_g_optimizer=generator_g_optimizer,\n",
    "                           generator_f_optimizer=generator_f_optimizer,\n",
    "                           discriminator_x_optimizer=discriminator_x_optimizer,\n",
    "                           discriminator_y_optimizer=discriminator_y_optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  print ('Latest checkpoint restored!!')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "def generate_images(model, test_input):\n",
    "    prediction = model(test_input)\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "\n",
    "    display_list = [test_input[0], prediction[0]]\n",
    "    title = ['Input Image', 'Predicted Image']\n",
    "\n",
    "    for i in range(2):\n",
    "        plt.subplot(1, 2, i+1)\n",
    "        plt.title(title[i])\n",
    "        # getting the pixel values between [0, 1] to plot it.\n",
    "        plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "@tf.function\n",
    "def train_step(real_x, real_y):\n",
    "    # persistent is set to True because the tape is used more than\n",
    "    # once to calculate the gradients.\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # Generator G translates X -> Y\n",
    "        # Generator F translates Y -> X.\n",
    "\n",
    "        fake_y = generator_g(real_x, training=True)\n",
    "        cycled_x = generator_f(fake_y, training=True)\n",
    "\n",
    "        fake_x = generator_f(real_y, training=True)\n",
    "        cycled_y = generator_g(fake_x, training=True)\n",
    "\n",
    "        # same_x and same_y are used for identity loss.\n",
    "        same_x = generator_f(real_x, training=True)\n",
    "        same_y = generator_g(real_y, training=True)\n",
    "\n",
    "        disc_real_x = discriminator_x(real_x, training=True)\n",
    "        disc_real_y = discriminator_y(real_y, training=True)\n",
    "\n",
    "        disc_fake_x = discriminator_x(fake_x, training=True)\n",
    "        disc_fake_y = discriminator_y(fake_y, training=True)\n",
    "\n",
    "        # calculate the loss\n",
    "        gen_g_loss = generator_loss(disc_fake_y)\n",
    "        gen_f_loss = generator_loss(disc_fake_x)\n",
    "\n",
    "        total_cycle_loss = calc_cycle_loss(real_x, cycled_x) + calc_cycle_loss(real_y, cycled_y)\n",
    "\n",
    "        # Total generator loss = adversarial loss + cycle loss\n",
    "        total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_loss(real_y, same_y)\n",
    "        total_gen_f_loss = gen_f_loss + total_cycle_loss + identity_loss(real_x, same_x)\n",
    "\n",
    "        disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)\n",
    "        disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n",
    "  \n",
    "    # Calculate the gradients for generator and discriminator\n",
    "    generator_g_gradients = tape.gradient(total_gen_g_loss, \n",
    "                                        generator_g.trainable_variables)\n",
    "    generator_f_gradients = tape.gradient(total_gen_f_loss, \n",
    "                                        generator_f.trainable_variables)\n",
    "\n",
    "    discriminator_x_gradients = tape.gradient(disc_x_loss, \n",
    "                                            discriminator_x.trainable_variables)\n",
    "    discriminator_y_gradients = tape.gradient(disc_y_loss, \n",
    "                                            discriminator_y.trainable_variables)\n",
    "\n",
    "    # Apply the gradients to the optimizer\n",
    "    generator_g_optimizer.apply_gradients(zip(generator_g_gradients, \n",
    "                                            generator_g.trainable_variables))\n",
    "\n",
    "    generator_f_optimizer.apply_gradients(zip(generator_f_gradients, \n",
    "                                            generator_f.trainable_variables))\n",
    "\n",
    "    discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients,\n",
    "                                                discriminator_x.trainable_variables))\n",
    "\n",
    "    discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients,\n",
    "                                                discriminator_y.trainable_variables))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    n = 0\n",
    "    for image_x, image_y in tf.data.Dataset.zip((train_horses, train_zebras)):\n",
    "        train_step(image_x, image_y)\n",
    "        if n % 10 == 0:\n",
    "            print ('.', end='')\n",
    "        n += 1\n",
    "\n",
    "    clear_output(wait=True)\n",
    "        # Using a consistent image (sample_horse) so that the progress of the model\n",
    "        # is clearly visible.\n",
    "    generate_images(generator_g, sample_horse)\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                                 ckpt_save_path))\n",
    "\n",
    "    print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n",
    "                                                              time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the trained model on the test dataset\n",
    "for inp in test_horses.take(5):\n",
    "    generate_images(generator_g, inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nod fijamos bien, vemos algunos fallso de transformación. Esto puede ser debido al colapso del modelo en el entrenamiento o que existen situaciones expeciales que no se han tomado en cuenta en el entrenamiento. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo CycleGAN también utiliza la arquitectura U-Net que espera entradas de imágenes de tamaño 256x256x3, por lo que es necesario realizar algunas adaptaciones para que funcione correctamente con imágenes de 258x540x1. \n",
    "\n",
    "Una posible solución es redimensionar las imágenes de entrada a un tamaño compatible con la arquitectura del modelo. En este caso redimensionaremos a 256x512x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.layers import ZeroPadding2D\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Define las rutas a los directorios de train y train_dirty\n",
    "PATH = 'docs'\n",
    "input_dirted  = 'docs/train'\n",
    "input_cleaned = 'docs/train_cleaned'\n",
    "\n",
    "# cargar y preprocesar los datasets de imágenes\n",
    "train_dirted  = tf.data.Dataset.list_files (str (input_dirted +'/*.png'))\n",
    "train_cleaned = tf.data.Dataset.list_files (str (input_cleaned +'/*.png'))\n",
    "\n",
    "for element in train_dirted:\n",
    "    print (element)\n",
    "    \n",
    "# Ajuste de parámetros para el conjunto de datos de documentos sucios\n",
    "BUFFER_SIZE     = 13\n",
    "BATCH_SIZE      = 1\n",
    "IMG_WIDTH       = 258\n",
    "IMG_HEIGHT      = 540\n",
    "M_IMG_WIDTH       = 256\n",
    "M_IMG_HEIGHT      = 512 \n",
    "OUTPUT_CHANNELS = 3\n",
    "IMG_CHANNELS    = 1\n",
    "\n",
    "R_IMG_WIDTH       = 288\n",
    "R_IMG_HEIGHT      = 570"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(image_file):\n",
    "    \n",
    "    # Read and decode an image file to a uint8 tensor\n",
    "    image = tf.io.read_file (image_file)\n",
    "    image = tf.io.decode_png (image,3)\n",
    "    #image = tf.repeat(image, 3, axis=-1) # Replicar el único canal en tres canales\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    " \n",
    "    return image\n",
    "\n",
    "def random_crop (image):\n",
    "    cropped_image = tf.image.random_crop (\n",
    "                          image, size = [M_IMG_WIDTH, M_IMG_HEIGHT,  3])\n",
    "\n",
    "    return cropped_image\n",
    "\n",
    "# normalizing the images to [-1, 1]\n",
    "def normalize (image):\n",
    "    ind = tf.cast (image, tf.float32)\n",
    "    if np.max(ind) <= 1.0 and np.min(ind) >= -1.0:\n",
    "        print(\"La imagen está dentro del rango [-1, 1]\")\n",
    "    else:\n",
    "        print(\"La imagen no está dentro del rango [-1, 1]\")\n",
    "        ind = (ind / 127.5) - 1\n",
    "    \n",
    "    return ind\n",
    "\n",
    "@tf.function\n",
    "def random_jitter (image):\n",
    "    # resizing to 286 x 286 x 3\n",
    "    ind = tf.image.resize (image, [R_IMG_WIDTH, R_IMG_HEIGHT],\n",
    "                          method = tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    " \n",
    "    # randomly cropping to 258 x 540 x 1\n",
    "    ind = random_crop (ind)\n",
    " \n",
    "    # random mirroring\n",
    "    ind = tf.image.random_flip_left_right (ind)\n",
    " \n",
    "    return ind\n",
    "\n",
    "\n",
    "def preprocess_image_train (input_image):\n",
    "    ind = load (input_image)\n",
    "    ind = random_jitter (ind)\n",
    "    # ind = normalize (ind) # Nohace falta, las imagenes ya están normalizadas entre {-1,1]\n",
    "    \n",
    "    return ind\n",
    "\n",
    "inp = load(str('docs/train/11.png'))\n",
    "\n",
    "print (inp.shape)\n",
    "\n",
    "\n",
    "# Casting to int for matplotlib to display the images\n",
    "plt.figure()\n",
    "plt.imshow(inp)\n",
    "plt.show ()\n",
    "\n",
    "inp2 = (normalize ( random_crop (load('docs/train/11.png'))) ) \n",
    "print (inp2.shape)\n",
    "plt.imshow ( inp2  )\n",
    "plt.show ()\n",
    "\n",
    "\n",
    "inp2 = (normalize ( random_crop (load('docs/train/11.png'))) ) \n",
    "print (inp2.shape)\n",
    "plt.imshow ( inp2  )\n",
    "plt.show ()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vemos que la iamgenes ya estan normalizadas entre [-1,1] por lo que no tenemos que normalizar.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dirted = train_dirted.cache().map(\n",
    "    preprocess_image_train, num_parallel_calls=AUTOTUNE).shuffle(\n",
    "    BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "train_cleaned = train_cleaned.cache().map(\n",
    "    preprocess_image_train, num_parallel_calls=AUTOTUNE).shuffle(\n",
    "    BUFFER_SIZE).batch(BATCH_SIZE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dirted  = next (iter (train_dirted))\n",
    "sample_cleaned = next (iter (train_cleaned))\n",
    "\n",
    "print (sample_dirted.shape)\n",
    "plt.imshow ( sample_dirted [0]   )\n",
    "plt.show ()\n",
    "\n",
    "\n",
    "print (sample_dirted.shape)\n",
    "plt.imshow ( sample_cleaned [0] )\n",
    "plt.show ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIX2PIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_g = pix2pix.unet_generator (OUTPUT_CHANNELS, norm_type='instancenorm')\n",
    "generator_f = pix2pix.unet_generator (OUTPUT_CHANNELS, norm_type='instancenorm')\n",
    " \n",
    "discriminator_x = pix2pix.discriminator ( norm_type = 'instancenorm', target = False)\n",
    "discriminator_y = pix2pix.discriminator ( norm_type = 'instancenorm', target = False)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "to_dirted  = generator_g (sample_dirted)\n",
    "to_cleaned = generator_f (sample_cleaned)\n",
    "\n",
    "plt.figure (figsize = (8, 8))\n",
    "contrast = 9\n",
    "\n",
    "imgs  = [ sample_dirted   , to_cleaned,  sample_cleaned   * 127.5 , to_dirted]\n",
    "title = ['Dirted', 'To Cleaned', 'Cleaned', 'To Dirted']\n",
    "\n",
    "for i in range (len (imgs)):\n",
    "    plt.subplot (2, 2, i+1)\n",
    "    plt.title (title [i])\n",
    " \n",
    "    if i % 2 == 0:\n",
    "        plt.imshow (imgs [i][0] * 0.5 + 0.5)\n",
    "    else:\n",
    "        plt.imshow (imgs [i][0] * 0.5 * contrast + 0.5)\n",
    "    \n",
    "plt.show ()\n",
    "\n",
    "plt.figure (figsize = (8, 8))\n",
    "\n",
    "plt.subplot (121)\n",
    "plt.title ('Is a real Cleaned?')\n",
    "plt.imshow (discriminator_y (sample_cleaned) [0, ..., -1], cmap = 'RdBu_r')\n",
    "\n",
    "plt.subplot (122)\n",
    "plt.title ('Is a real Dirted?')\n",
    "plt.imshow (discriminator_x (sample_dirted) [0, ..., -1], cmap = 'RdBu_r')\n",
    "\n",
    "plt.show ()\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 10\n",
    "\n",
    "loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real, generated):\n",
    "  real_loss = loss_obj(tf.ones_like(real), real)\n",
    "\n",
    "  generated_loss = loss_obj(tf.zeros_like(generated), generated)\n",
    "\n",
    "  total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "  return total_disc_loss * 0.5\n",
    "\n",
    "def generator_loss(generated):\n",
    "  return loss_obj(tf.ones_like(generated), generated)\n",
    "\n",
    "def calc_cycle_loss(real_image, cycled_image):\n",
    "  loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
    "  \n",
    "  return LAMBDA * loss1\n",
    "\n",
    "def identity_loss(real_image, same_image):\n",
    "  loss = tf.reduce_mean(tf.abs(real_image - same_image))\n",
    "  return LAMBDA * 0.5 * loss\n",
    "\n",
    "generator_g_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "generator_f_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "discriminator_x_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train_docs\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(generator_g=generator_g,\n",
    "                           generator_f=generator_f,\n",
    "                           discriminator_x=discriminator_x,\n",
    "                           discriminator_y=discriminator_y,\n",
    "                           generator_g_optimizer=generator_g_optimizer,\n",
    "                           generator_f_optimizer=generator_f_optimizer,\n",
    "                           discriminator_x_optimizer=discriminator_x_optimizer,\n",
    "                           discriminator_y_optimizer=discriminator_y_optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 40\n",
    "\n",
    "def generate_images (model, test_input):\n",
    "    prediction = model (test_input)\n",
    "    print (prediction.shape)\n",
    "    plt.figure (figsize = (12,12))\n",
    "     \n",
    "    #test_input  = (test_input + 1)* 127.5\n",
    "    #prediction  = (prediction + 1)* 127.5\n",
    "    \n",
    "    display_list = [test_input [0], prediction [0]]\n",
    "    title = ['Input Image', 'Predicted Image']\n",
    "\n",
    "    for i in range (2):\n",
    "        plt.subplot (1, 2, i+1)\n",
    "        plt.title (title[i])\n",
    "        # getting the pixel values between [0, 1] to plot it.\n",
    "        plt.imshow (display_list[i]*0.5+0.5)\n",
    "        plt.axis ('off')\n",
    "        plt.show ()\n",
    "\n",
    "@tf.function\n",
    "def train_step(real_x, real_y):\n",
    "    # persistent is set to True because the tape is used more than\n",
    "    # once to calculate the gradients.\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # Generator G translates X -> Y\n",
    "        # Generator F translates Y -> X.\n",
    "\n",
    "        fake_y   = generator_g(real_x, training=True)\n",
    "        cycled_x = generator_f(fake_y, training=True)\n",
    "\n",
    "        fake_x   = generator_f(real_y, training=True)\n",
    "        cycled_y = generator_g(fake_x, training=True)\n",
    "\n",
    "        # same_x and same_y are used for identity loss.\n",
    "        same_x = generator_f(real_x, training=True)\n",
    "        same_y = generator_g(real_y, training=True)\n",
    "\n",
    "        disc_real_x = discriminator_x(real_x, training=True)\n",
    "        disc_real_y = discriminator_y(real_y, training=True)\n",
    "\n",
    "        disc_fake_x = discriminator_x(fake_x, training=True)\n",
    "        disc_fake_y = discriminator_y(fake_y, training=True)\n",
    "\n",
    "        # calculate the loss\n",
    "        gen_g_loss = generator_loss(disc_fake_y)\n",
    "        gen_f_loss = generator_loss(disc_fake_x)\n",
    "\n",
    "        total_cycle_loss = calc_cycle_loss(real_x, cycled_x) + calc_cycle_loss(real_y, cycled_y)\n",
    "\n",
    "        # Total generator loss = adversarial loss + cycle loss\n",
    "        total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_loss(real_y, same_y)\n",
    "        total_gen_f_loss = gen_f_loss + total_cycle_loss + identity_loss(real_x, same_x)\n",
    "\n",
    "        disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)\n",
    "        disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n",
    "  \n",
    "    # Calculate the gradients for generator and discriminator\n",
    "    generator_g_gradients = tape.gradient(total_gen_g_loss, \n",
    "                                        generator_g.trainable_variables)\n",
    "    generator_f_gradients = tape.gradient(total_gen_f_loss, \n",
    "                                        generator_f.trainable_variables)\n",
    "\n",
    "    discriminator_x_gradients = tape.gradient(disc_x_loss, \n",
    "                                            discriminator_x.trainable_variables)\n",
    "    discriminator_y_gradients = tape.gradient(disc_y_loss, \n",
    "                                            discriminator_y.trainable_variables)\n",
    "\n",
    "    # Apply the gradients to the optimizer\n",
    "    generator_g_optimizer.apply_gradients(zip(generator_g_gradients, \n",
    "                                            generator_g.trainable_variables))\n",
    "\n",
    "    generator_f_optimizer.apply_gradients(zip(generator_f_gradients, \n",
    "                                            generator_f.trainable_variables))\n",
    "\n",
    "    discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients,\n",
    "                                                discriminator_x.trainable_variables))\n",
    "\n",
    "    discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients,\n",
    "                                                discriminator_y.trainable_variables))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    n = 0\n",
    "    for image_x, image_y in tf.data.Dataset.zip((train_dirted, train_cleaned)):\n",
    "   \n",
    "        train_step (image_x, image_y)\n",
    "        if n % 10 == 0:\n",
    "            print ('.', end='')\n",
    "        n += 1\n",
    "\n",
    "    clear_output (wait=True)\n",
    "        # Using a consistent image (sample_dirted) so that the progress of the model\n",
    "        # is clearly visible.\n",
    "    print(' (*) Limpiando documentos:')\n",
    "    generate_images(generator_g, sample_dirted)\n",
    "    #print(' (*) Ensuciando documentos:')\n",
    "    #generate_images(generator_f, sample_cleaned)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                                 ckpt_save_path))\n",
    "\n",
    "    print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n",
    "                                                              time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vemos como la imagen predicha es una documento limpio.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test : Dirted --> cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for inp in train_dirted.take(5):\n",
    "    generate_images(generator_g, inp  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test : cleaned  --> Dirted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for inp in train_cleaned.take(5):\n",
    "    generate_images(generator_f, inp  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobmo que los documentos sucios los limpia con generator_g. Y los documentos limpios lso ensucia con genertor_f"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "P3_parte_2_GANs_condicionales.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
